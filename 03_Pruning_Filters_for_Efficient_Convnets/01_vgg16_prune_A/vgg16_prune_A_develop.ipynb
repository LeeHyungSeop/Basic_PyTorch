{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. train되어 있는 model에 s_j 기준 내림차순 정렬\n",
    "2. 하위 channel 제거\n",
    "3. 40 epoch retraining\n",
    "4. Test Accuracy, #FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 16:51:11.341888: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-06 16:51:11.543223: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from pytorch_model_summary import summary\n",
    "import torch.nn.init as init\n",
    "import kornia\n",
    "import math\n",
    "import torch.profiler as profiler\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/hslee/Desktop/Embedded_AI/PyTorch_Tutorials/03_Pruning_Filters_for_Efficient_Convnets/')\n",
    "from architecture import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. One shot pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG16_BN()\n",
    "checkpoint = torch.load('../00_vgg16_baseline_exp1/checkpoint/best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv 1 : torch.Size([64, 3, 3, 3])\n",
      "conv 2 : torch.Size([64, 64, 3, 3])\n",
      "conv 3 : torch.Size([128, 64, 3, 3])\n",
      "conv 4 : torch.Size([128, 128, 3, 3])\n",
      "conv 5 : torch.Size([256, 128, 3, 3])\n",
      "conv 6 : torch.Size([256, 256, 3, 3])\n",
      "conv 7 : torch.Size([256, 256, 3, 3])\n",
      "conv 8 : torch.Size([512, 256, 3, 3])\n",
      "conv 9 : torch.Size([512, 512, 3, 3])\n",
      "conv 10 : torch.Size([512, 512, 3, 3])\n",
      "conv 11 : torch.Size([512, 512, 3, 3])\n",
      "conv 12 : torch.Size([512, 512, 3, 3])\n",
      "conv 13 : torch.Size([512, 512, 3, 3])\n",
      "fc : torch.Size([512, 512])\n",
      "fc : torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "layer_idx = 1\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        print(f\"conv {layer_idx} : {layer.weight.data.shape}\")\n",
    "        layer_idx += 1\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        print(f\"fc : {layer.weight.data.shape}\")\n",
    "        layer_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): ReLU(inplace=True)\n",
      ")\n",
      "torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "layer = getattr(model, f\"fc1\")\n",
    "print(layer)\n",
    "\n",
    "fc_layer = layer[1]\n",
    "print(fc_layer.weight.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================  conv1  ========================================\n",
      "pruned_rate : 50%\n",
      "num_prune_channels : 32\n",
      "weight.shape : torch.Size([64, 3, 3, 3])\n",
      "bias.shape : torch.Size([64])\n",
      "bn_gamma.shape : torch.Size([64])\n",
      "bn_beta.shape : torch.Size([64])\n",
      "bn_running_mean.shape : torch.Size([64])\n",
      "bn_running_var.shape : torch.Size([64])\n",
      "sorted_weight_indices : tensor([30, 24, 32, 22, 47, 41, 49, 53, 51, 56, 23,  0, 15, 57, 50,  7, 58, 36,\n",
      "         2,  4, 12, 31, 17, 16, 63, 21,  1, 33, 48,  9, 43, 13, 37, 52, 34, 27,\n",
      "        10, 62, 60, 46, 59, 39, 18, 40, 55, 29,  3,  8, 19, 54, 35, 20, 28,  6,\n",
      "        11, 44, 14, 61, 25, 38, 45,  5, 26, 42])\n",
      "saving_filter_idices : tensor([30, 24, 32, 22, 47, 41, 49, 53, 51, 56, 23,  0, 15, 57, 50,  7, 58, 36,\n",
      "         2,  4, 12, 31, 17, 16, 63, 21,  1, 33, 48,  9, 43, 13])\n",
      "pruned_weight.shape : torch.Size([32, 3, 3, 3])\n",
      "pruned_bias.shape : torch.Size([32])\n",
      "pruned_bn_gamma.shape : torch.Size([32])\n",
      "pruned_bn_beta.shape : torch.Size([32])\n",
      "pruned_bn_running_mean.shape : torch.Size([32])\n",
      "pruned_bn_running_var.shape : torch.Size([32])\n",
      "pruned_next_weight.shape : torch.Size([64, 32, 3, 3])\n",
      "========================================  conv2  ========================================\n",
      "pruned_rate : 0%\n",
      "no filter pruned\n",
      "========================================  conv3  ========================================\n",
      "pruned_rate : 0%\n",
      "no filter pruned\n",
      "========================================  conv4  ========================================\n",
      "pruned_rate : 0%\n",
      "no filter pruned\n",
      "========================================  conv5  ========================================\n",
      "pruned_rate : 0%\n",
      "no filter pruned\n",
      "========================================  conv6  ========================================\n",
      "pruned_rate : 0%\n",
      "no filter pruned\n",
      "========================================  conv7  ========================================\n",
      "pruned_rate : 0%\n",
      "no filter pruned\n",
      "========================================  conv8  ========================================\n",
      "pruned_rate : 50%\n",
      "num_prune_channels : 256\n",
      "weight.shape : torch.Size([512, 256, 3, 3])\n",
      "bias.shape : torch.Size([512])\n",
      "bn_gamma.shape : torch.Size([512])\n",
      "bn_beta.shape : torch.Size([512])\n",
      "bn_running_mean.shape : torch.Size([512])\n",
      "bn_running_var.shape : torch.Size([512])\n",
      "sorted_weight_indices : tensor([493, 503, 340, 370, 265, 147, 255, 439, 335,   3, 135,  26, 360, 347,\n",
      "        405, 208, 456, 398, 176,  97, 132, 476,  21, 477,  46, 246, 234, 322,\n",
      "        371, 159, 278, 355, 189, 136, 239, 274, 161, 269, 124, 378,  70,  86,\n",
      "        106,  76, 182,  74, 187,  22, 112, 207, 353, 441, 458,  59, 144, 430,\n",
      "        175,  98, 482, 309, 470, 367, 321, 317, 242, 403, 284,  84, 169,  83,\n",
      "        155, 473, 268, 215,  94, 387, 217, 363, 437, 150,  30, 248, 376,  90,\n",
      "        465, 171,  23, 432, 313,  63, 461, 395,  66, 307, 419,  44,  34, 190,\n",
      "        228, 392, 104,  56, 494, 204,  93,  57, 488,  81, 337, 393, 359, 251,\n",
      "        197, 127,  40, 230, 213, 158, 448, 126, 261, 184, 270, 447, 111, 103,\n",
      "         45, 373, 272, 481, 177,  10, 315, 134, 160, 218,  16, 350, 502,  55,\n",
      "        424, 237, 366,  99, 173, 118, 348, 435,  41, 295, 291, 436, 224, 364,\n",
      "        156,  29, 459, 389, 341, 256, 162, 356,  17, 142, 117, 475, 209, 497,\n",
      "        275, 128, 361, 490, 280, 264, 452, 107, 186, 250,   5, 318,  20, 396,\n",
      "        368, 231, 483, 152, 170, 428, 236,  28, 443, 110, 422,  60, 129, 415,\n",
      "         69, 420, 297,  67, 388, 257, 384, 206, 140, 286,   9,  15, 444, 351,\n",
      "        372, 450, 504, 164, 345, 401, 414, 467, 298, 108,  82,  53, 219, 386,\n",
      "        174, 271, 247,  13, 180, 105, 402, 148, 354, 455, 357, 165, 472,  68,\n",
      "        468, 499,  24, 332,  12,   7, 449,  25, 125, 399, 304, 500, 253, 214,\n",
      "        259, 352, 330, 229, 168, 464,  42, 311, 288,  65, 349, 305, 146, 491,\n",
      "        369, 109, 178, 191,  85, 339, 149, 233, 509, 469, 451, 130, 487, 485,\n",
      "        262,  78, 323, 334, 429, 260, 381, 198, 123, 166, 181, 221, 446, 338,\n",
      "         79, 263,  96, 276,  31, 119, 122,  54, 344,  92,  58, 417,  64, 258,\n",
      "        235, 153,  50,  11, 139, 427, 249, 157, 154, 290, 501, 343, 329, 431,\n",
      "        362, 225, 425, 506,  14, 408, 216,  80, 240,   1,   6,  73,  52, 336,\n",
      "        314, 331, 143, 426, 434, 474,   0, 326, 303, 489,  19, 457,  72, 380,\n",
      "        102, 114,   8, 374,  35, 279,  18, 101, 440, 453, 238, 328, 202, 312,\n",
      "        199, 327, 281, 192, 413, 445, 138, 167, 277, 245,  27, 212, 296, 375,\n",
      "        227,  32, 394,  51, 115, 163, 463,  43,  87, 306, 505, 241, 462, 416,\n",
      "        131, 145,  38, 391, 404,  37, 418, 289, 442, 460, 232, 406, 407, 116,\n",
      "        400, 358, 193, 196, 423, 342,  62, 267, 172,  49, 409, 510, 195, 480,\n",
      "        299, 316, 308,  91, 205, 200, 438, 266,  88, 282, 293, 346, 383, 495,\n",
      "        243, 287, 397, 379, 390,  33, 211, 141, 325,  89, 310, 222,   4, 220,\n",
      "         77, 100,  61, 151, 294, 511, 188, 333, 254, 292,  75, 479, 113,  36,\n",
      "         47, 498, 492, 484, 412, 320, 133, 252, 301, 411,  39, 300, 421, 302,\n",
      "        185, 377, 194, 121, 454, 285, 433,  71, 382, 319, 496, 486, 273, 283,\n",
      "        478, 203, 226, 507, 324,  48, 471, 466, 120, 365, 183,  95, 210,   2,\n",
      "        410, 385, 223, 137, 508, 244, 179, 201])\n",
      "saving_filter_idices : tensor([493, 503, 340, 370, 265, 147, 255, 439, 335,   3, 135,  26, 360, 347,\n",
      "        405, 208, 456, 398, 176,  97, 132, 476,  21, 477,  46, 246, 234, 322,\n",
      "        371, 159, 278, 355, 189, 136, 239, 274, 161, 269, 124, 378,  70,  86,\n",
      "        106,  76, 182,  74, 187,  22, 112, 207, 353, 441, 458,  59, 144, 430,\n",
      "        175,  98, 482, 309, 470, 367, 321, 317, 242, 403, 284,  84, 169,  83,\n",
      "        155, 473, 268, 215,  94, 387, 217, 363, 437, 150,  30, 248, 376,  90,\n",
      "        465, 171,  23, 432, 313,  63, 461, 395,  66, 307, 419,  44,  34, 190,\n",
      "        228, 392, 104,  56, 494, 204,  93,  57, 488,  81, 337, 393, 359, 251,\n",
      "        197, 127,  40, 230, 213, 158, 448, 126, 261, 184, 270, 447, 111, 103,\n",
      "         45, 373, 272, 481, 177,  10, 315, 134, 160, 218,  16, 350, 502,  55,\n",
      "        424, 237, 366,  99, 173, 118, 348, 435,  41, 295, 291, 436, 224, 364,\n",
      "        156,  29, 459, 389, 341, 256, 162, 356,  17, 142, 117, 475, 209, 497,\n",
      "        275, 128, 361, 490, 280, 264, 452, 107, 186, 250,   5, 318,  20, 396,\n",
      "        368, 231, 483, 152, 170, 428, 236,  28, 443, 110, 422,  60, 129, 415,\n",
      "         69, 420, 297,  67, 388, 257, 384, 206, 140, 286,   9,  15, 444, 351,\n",
      "        372, 450, 504, 164, 345, 401, 414, 467, 298, 108,  82,  53, 219, 386,\n",
      "        174, 271, 247,  13, 180, 105, 402, 148, 354, 455, 357, 165, 472,  68,\n",
      "        468, 499,  24, 332,  12,   7, 449,  25, 125, 399, 304, 500, 253, 214,\n",
      "        259, 352, 330, 229])\n",
      "pruned_weight.shape : torch.Size([256, 256, 3, 3])\n",
      "pruned_bias.shape : torch.Size([256])\n",
      "pruned_bn_gamma.shape : torch.Size([256])\n",
      "pruned_bn_beta.shape : torch.Size([256])\n",
      "pruned_bn_running_mean.shape : torch.Size([256])\n",
      "pruned_bn_running_var.shape : torch.Size([256])\n",
      "pruned_next_weight.shape : torch.Size([512, 256, 3, 3])\n",
      "========================================  conv9  ========================================\n",
      "pruned_rate : 50%\n",
      "num_prune_channels : 256\n",
      "weight.shape : torch.Size([512, 256, 3, 3])\n",
      "bias.shape : torch.Size([512])\n",
      "bn_gamma.shape : torch.Size([512])\n",
      "bn_beta.shape : torch.Size([512])\n",
      "bn_running_mean.shape : torch.Size([512])\n",
      "bn_running_var.shape : torch.Size([512])\n",
      "sorted_weight_indices : tensor([370, 424, 245, 268, 312, 109, 316, 297,  95, 413, 241, 244, 117, 160,\n",
      "          6,  60, 254, 201, 375, 445,  33, 119, 451, 149, 309, 275, 453, 384,\n",
      "         86, 202, 183, 456, 488, 143, 162,  46, 212, 347, 463, 276,  96, 272,\n",
      "        140, 181,  15, 108, 194,  78, 482, 409, 314, 442,   7, 280,  98, 214,\n",
      "        240, 300, 301, 302, 231, 416, 461,  90,  85, 349,  77, 433, 216, 226,\n",
      "        155, 483, 444, 131, 217,  55, 173, 407,  76,  81, 303, 282, 481, 469,\n",
      "        278, 230, 311, 253, 152, 393, 465, 340, 228, 487, 394, 101, 400, 509,\n",
      "         50,   1, 196, 345, 222, 178, 428, 281, 154,  74, 251, 423, 498, 426,\n",
      "         56, 136, 372, 233, 255, 168,  71,   5, 352, 495, 328,   4, 187, 369,\n",
      "        157, 293,  80,  37, 334,  36, 147,  99, 502, 346,  17, 171,  97, 292,\n",
      "        163, 223, 294, 227, 452, 269, 103, 113,  52, 503, 412,  16, 165, 305,\n",
      "         73,   2,  18, 506, 325, 247, 310, 353, 447,  64, 246, 414, 121, 236,\n",
      "        172, 159, 166, 507, 205, 313, 430, 410, 368,  35,  58, 378, 307, 324,\n",
      "        484, 169,  65, 174, 118,  25,  38, 192, 208,  53,  21,   3,  94, 123,\n",
      "          9, 342, 125,  69, 130, 485, 350, 486, 437, 504, 478, 285, 197,  39,\n",
      "          8, 170, 459, 257, 266, 365, 467, 215, 496, 351, 184, 322, 448, 235,\n",
      "         19, 319, 464,  49,  54, 234, 411, 335, 359, 145, 385, 105, 388, 256,\n",
      "         72, 401,  30, 344, 455, 209,  13,  40, 392, 438,  63, 114, 274, 284,\n",
      "         12, 458, 115, 466, 146, 243, 102,  93, 112, 164, 439, 182, 441, 327,\n",
      "        471,  47, 462,  31, 283, 219, 440, 398, 139, 265, 225, 408, 366, 402,\n",
      "        298, 138, 406, 158,  42,  27, 468, 264, 417, 100, 151, 308, 421, 200,\n",
      "        267, 232, 341,  67, 206, 371, 323,  28, 354, 258, 189, 156, 331, 315,\n",
      "        475, 491,  79, 477, 367, 449, 175,  89, 390, 299, 129, 198,  24, 239,\n",
      "        450, 126, 460, 120, 238, 180, 286,  88, 317,  45, 508, 321, 435, 229,\n",
      "        195, 415, 185, 290, 318, 404, 403, 291, 427, 110, 499, 262, 104, 142,\n",
      "        220, 473, 218,  75, 457, 186, 381, 337, 287,  61,  70, 363, 333, 124,\n",
      "         84, 395, 270, 510, 357, 137, 141, 364, 221,  68, 355,  92, 289, 339,\n",
      "        391, 474,  48, 259, 500, 188, 295, 144, 436, 107, 193, 380, 199, 122,\n",
      "        383, 116, 489,  29,  10,  44, 479, 279, 494,  14, 470, 306, 263, 389,\n",
      "         59,  66, 326, 261, 250, 418,  57, 127, 329, 343, 419,  41, 360, 288,\n",
      "        132, 237, 167, 135, 374, 134, 376, 387, 399, 432, 422, 472, 497, 425,\n",
      "        248, 273,   0, 429,  43, 277, 405, 296, 249, 320, 210, 420, 128,  26,\n",
      "        161,  34, 356, 191, 511,  51, 106, 190, 252, 505, 492,  91, 454, 431,\n",
      "         62, 213,  82, 150, 203, 476, 207, 338,  20, 379, 443, 242, 348, 397,\n",
      "        434, 336, 396, 133, 386, 177, 358,  22, 211,  87, 332, 204, 361, 304,\n",
      "        260, 373, 148, 501, 382, 480, 271, 176, 377, 179, 330, 153, 490, 446,\n",
      "         32,  83, 224, 111,  23,  11, 362, 493])\n",
      "saving_filter_idices : tensor([370, 424, 245, 268, 312, 109, 316, 297,  95, 413, 241, 244, 117, 160,\n",
      "          6,  60, 254, 201, 375, 445,  33, 119, 451, 149, 309, 275, 453, 384,\n",
      "         86, 202, 183, 456, 488, 143, 162,  46, 212, 347, 463, 276,  96, 272,\n",
      "        140, 181,  15, 108, 194,  78, 482, 409, 314, 442,   7, 280,  98, 214,\n",
      "        240, 300, 301, 302, 231, 416, 461,  90,  85, 349,  77, 433, 216, 226,\n",
      "        155, 483, 444, 131, 217,  55, 173, 407,  76,  81, 303, 282, 481, 469,\n",
      "        278, 230, 311, 253, 152, 393, 465, 340, 228, 487, 394, 101, 400, 509,\n",
      "         50,   1, 196, 345, 222, 178, 428, 281, 154,  74, 251, 423, 498, 426,\n",
      "         56, 136, 372, 233, 255, 168,  71,   5, 352, 495, 328,   4, 187, 369,\n",
      "        157, 293,  80,  37, 334,  36, 147,  99, 502, 346,  17, 171,  97, 292,\n",
      "        163, 223, 294, 227, 452, 269, 103, 113,  52, 503, 412,  16, 165, 305,\n",
      "         73,   2,  18, 506, 325, 247, 310, 353, 447,  64, 246, 414, 121, 236,\n",
      "        172, 159, 166, 507, 205, 313, 430, 410, 368,  35,  58, 378, 307, 324,\n",
      "        484, 169,  65, 174, 118,  25,  38, 192, 208,  53,  21,   3,  94, 123,\n",
      "          9, 342, 125,  69, 130, 485, 350, 486, 437, 504, 478, 285, 197,  39,\n",
      "          8, 170, 459, 257, 266, 365, 467, 215, 496, 351, 184, 322, 448, 235,\n",
      "         19, 319, 464,  49,  54, 234, 411, 335, 359, 145, 385, 105, 388, 256,\n",
      "         72, 401,  30, 344, 455, 209,  13,  40, 392, 438,  63, 114, 274, 284,\n",
      "         12, 458, 115, 466])\n",
      "pruned_weight.shape : torch.Size([256, 256, 3, 3])\n",
      "pruned_bias.shape : torch.Size([256])\n",
      "pruned_bn_gamma.shape : torch.Size([256])\n",
      "pruned_bn_beta.shape : torch.Size([256])\n",
      "pruned_bn_running_mean.shape : torch.Size([256])\n",
      "pruned_bn_running_var.shape : torch.Size([256])\n",
      "pruned_next_weight.shape : torch.Size([512, 256, 3, 3])\n",
      "========================================  conv10  ========================================\n",
      "pruned_rate : 50%\n",
      "num_prune_channels : 256\n",
      "weight.shape : torch.Size([512, 256, 3, 3])\n",
      "bias.shape : torch.Size([512])\n",
      "bn_gamma.shape : torch.Size([512])\n",
      "bn_beta.shape : torch.Size([512])\n",
      "bn_running_mean.shape : torch.Size([512])\n",
      "bn_running_var.shape : torch.Size([512])\n",
      "sorted_weight_indices : tensor([333, 325, 156, 221, 500,   9, 442, 269, 334,  12, 385, 479,  74,  80,\n",
      "        140, 220, 187, 170, 258, 120, 130, 382,  56, 203, 336, 265, 295, 285,\n",
      "        413, 457, 118, 281,  63, 273, 353, 256, 126, 487, 205,  35,  33, 311,\n",
      "        508,  53,  50, 383, 153, 294, 389, 210, 343, 211, 217, 248, 191, 374,\n",
      "        366, 376, 175, 379, 489, 352, 257, 247, 473, 464, 420, 347, 177, 454,\n",
      "        423, 125, 259, 390,   4, 469,  31, 440, 261, 459, 219, 252, 432, 218,\n",
      "        306, 138,  79, 224,  29, 498, 245,   2, 226, 303, 456, 109, 332,  41,\n",
      "        168, 360, 122, 264, 188, 395, 148, 236, 410, 511, 453,  13,  64, 393,\n",
      "        342, 169, 277, 495, 475, 369, 368, 397, 155, 129, 392,  49, 267, 181,\n",
      "        288,  17, 111, 466, 107,  87, 114, 193, 465, 398, 452, 443, 380, 326,\n",
      "        301, 135, 134, 208, 329, 490, 478, 502, 322, 240, 123, 439, 144, 357,\n",
      "         54, 350, 242, 289, 371, 189, 331, 437, 182, 287, 137,  69,  88, 482,\n",
      "        450, 400, 145, 106,  36, 174, 234, 223, 408, 262,  16, 330, 195,  19,\n",
      "        231, 447,  22, 412, 445, 184, 405,   5, 216,  14,  42, 237, 147,  67,\n",
      "         73, 403,  81, 100,  59, 238, 196, 485,  82, 318, 433, 418, 161, 198,\n",
      "        192,  58,  95, 427, 363, 463, 436, 425, 142, 509, 377, 348, 349, 243,\n",
      "        167, 131,  23, 506,  34, 101, 430, 503,  90, 263, 365, 146, 233, 488,\n",
      "         77, 504,  15, 468, 312, 356,  99, 388, 212, 378,  75, 116, 251, 337,\n",
      "        492,   3, 414, 108, 183,  89, 300,   6, 133,  66, 213, 449,  84, 340,\n",
      "         72, 136, 458, 304,  25, 510,  44, 199, 266, 185, 387, 250,  78, 404,\n",
      "        149,  26, 421, 299,  52, 474, 448,  85,  37,  21, 159,  61, 151, 128,\n",
      "        290, 339, 344, 402,  94, 507, 166,  46, 293, 280, 113, 444, 406, 150,\n",
      "        462, 345, 141, 115, 323, 241, 207,  62, 297, 254, 375, 384, 486,  76,\n",
      "         11, 505, 298, 328, 286, 190,  40, 255, 370, 451, 394, 143, 484, 282,\n",
      "        103, 171,  38, 313,  93, 416, 246, 315, 496, 152,   8, 401, 222,  98,\n",
      "        477, 481, 229, 232, 110,  47, 346, 309, 163, 179, 446, 399, 391, 186,\n",
      "         27, 431, 355, 127, 438, 235, 471,  60, 476, 284, 104, 362,  55, 278,\n",
      "        172,  65, 271, 173, 180, 441, 202, 372, 162, 310, 386, 351, 422, 480,\n",
      "        319,  10, 291, 497, 305, 429,  30, 121, 493, 426, 308, 194, 178, 274,\n",
      "        157, 327, 112, 225, 455, 472, 296,  91, 275, 176,  24, 367, 461,  96,\n",
      "        483, 239, 460, 164, 407,  45, 320, 228, 260, 381, 324, 361,  86, 117,\n",
      "        491, 272, 415, 230,   0, 244, 197, 227,  43, 417, 354, 338, 411, 467,\n",
      "        358, 124, 428, 139,  57,  20,  71,  83, 341,  68, 206, 200, 209, 501,\n",
      "         51, 119, 276, 396,  28, 499, 321, 364,   7, 268, 249,  32, 158, 494,\n",
      "        154, 201,  39,  48, 165, 302, 359, 409,   1, 314, 317, 214, 160, 307,\n",
      "         70,  92, 424, 335, 102, 292,  18, 132, 270, 283, 253, 419, 316, 279,\n",
      "        435, 105, 204, 470, 434,  97, 373, 215])\n",
      "saving_filter_idices : tensor([333, 325, 156, 221, 500,   9, 442, 269, 334,  12, 385, 479,  74,  80,\n",
      "        140, 220, 187, 170, 258, 120, 130, 382,  56, 203, 336, 265, 295, 285,\n",
      "        413, 457, 118, 281,  63, 273, 353, 256, 126, 487, 205,  35,  33, 311,\n",
      "        508,  53,  50, 383, 153, 294, 389, 210, 343, 211, 217, 248, 191, 374,\n",
      "        366, 376, 175, 379, 489, 352, 257, 247, 473, 464, 420, 347, 177, 454,\n",
      "        423, 125, 259, 390,   4, 469,  31, 440, 261, 459, 219, 252, 432, 218,\n",
      "        306, 138,  79, 224,  29, 498, 245,   2, 226, 303, 456, 109, 332,  41,\n",
      "        168, 360, 122, 264, 188, 395, 148, 236, 410, 511, 453,  13,  64, 393,\n",
      "        342, 169, 277, 495, 475, 369, 368, 397, 155, 129, 392,  49, 267, 181,\n",
      "        288,  17, 111, 466, 107,  87, 114, 193, 465, 398, 452, 443, 380, 326,\n",
      "        301, 135, 134, 208, 329, 490, 478, 502, 322, 240, 123, 439, 144, 357,\n",
      "         54, 350, 242, 289, 371, 189, 331, 437, 182, 287, 137,  69,  88, 482,\n",
      "        450, 400, 145, 106,  36, 174, 234, 223, 408, 262,  16, 330, 195,  19,\n",
      "        231, 447,  22, 412, 445, 184, 405,   5, 216,  14,  42, 237, 147,  67,\n",
      "         73, 403,  81, 100,  59, 238, 196, 485,  82, 318, 433, 418, 161, 198,\n",
      "        192,  58,  95, 427, 363, 463, 436, 425, 142, 509, 377, 348, 349, 243,\n",
      "        167, 131,  23, 506,  34, 101, 430, 503,  90, 263, 365, 146, 233, 488,\n",
      "         77, 504,  15, 468, 312, 356,  99, 388, 212, 378,  75, 116, 251, 337,\n",
      "        492,   3, 414, 108])\n",
      "pruned_weight.shape : torch.Size([256, 256, 3, 3])\n",
      "pruned_bias.shape : torch.Size([256])\n",
      "pruned_bn_gamma.shape : torch.Size([256])\n",
      "pruned_bn_beta.shape : torch.Size([256])\n",
      "pruned_bn_running_mean.shape : torch.Size([256])\n",
      "pruned_bn_running_var.shape : torch.Size([256])\n",
      "pruned_next_weight.shape : torch.Size([512, 256, 3, 3])\n",
      "========================================  conv11  ========================================\n",
      "pruned_rate : 50%\n",
      "num_prune_channels : 256\n",
      "weight.shape : torch.Size([512, 256, 3, 3])\n",
      "bias.shape : torch.Size([512])\n",
      "bn_gamma.shape : torch.Size([512])\n",
      "bn_beta.shape : torch.Size([512])\n",
      "bn_running_mean.shape : torch.Size([512])\n",
      "bn_running_var.shape : torch.Size([512])\n",
      "sorted_weight_indices : tensor([ 31, 487, 352, 341, 463, 219, 222, 349, 281, 333, 210,  91, 485, 457,\n",
      "        180, 504, 488, 490, 304, 273, 125, 407, 310, 181, 115,  62,  35, 346,\n",
      "        356,  58, 393, 255,  89, 230, 388, 315, 340, 371, 211, 144, 491, 173,\n",
      "        338,  65, 259, 110, 473, 445, 505, 216, 309,  67, 114, 450,  45, 108,\n",
      "        155, 102, 409, 111,  94, 193, 332, 119, 166, 479, 447,  12, 510, 454,\n",
      "        401, 467, 227, 325, 337, 402,  72, 321, 250, 462, 152, 184, 376,  79,\n",
      "         13, 223, 201, 433, 392,  42, 291, 233, 300,  32, 444, 334, 244, 411,\n",
      "        252, 292,  33, 448, 501, 133,  57, 169,  18, 288,  61, 164, 353, 449,\n",
      "        139,  86,  75, 317,  26,  92,  52, 422, 145,   2, 400,   3, 498, 270,\n",
      "         21, 100, 484, 283, 205, 239, 280, 182, 241,  70, 418, 103,  69, 363,\n",
      "        378, 295,  15,  28, 499, 303, 104, 129,  41, 446, 458,   5,  30,  80,\n",
      "        368, 472, 394, 354,  59,  34, 386, 176, 195, 137, 150,  63, 260, 156,\n",
      "        263, 269, 477, 414,   1,  19, 272, 500, 330, 360, 170, 361, 461, 397,\n",
      "        496, 347, 157, 282, 459, 358, 415, 294,  90, 511, 440,  76, 236, 194,\n",
      "        159, 326, 417, 151, 482,  78, 437, 168, 377, 271, 218, 208, 140,   0,\n",
      "        486,  17,   8,  71, 234, 480, 105, 187,  40, 342,  87, 174, 217, 225,\n",
      "        320, 245, 384, 165, 328, 302, 395, 147, 426, 162, 443, 365, 478, 307,\n",
      "        372, 483, 257, 493, 203, 154, 183, 316, 423,  16, 258,   4, 364, 153,\n",
      "        177, 318,  22, 235, 314, 262, 127, 167,  49, 408, 237,  83,  54, 460,\n",
      "        293, 106, 442, 475, 146, 275, 214,  85, 428, 312, 507, 502,  48,   9,\n",
      "        464, 141, 226, 112,  97,  66, 431, 336, 306, 385,  44, 253, 382, 186,\n",
      "        120, 430, 284, 107, 506, 246, 416, 160, 436,   6, 279,  60,  99, 124,\n",
      "         84, 434,  68, 381, 199, 276,  29, 481, 161, 468, 424, 215, 305,  47,\n",
      "        374, 380, 366,  14, 240,  38, 375, 322, 403, 331,  53, 278, 251, 220,\n",
      "        131, 242, 412, 313, 379,   7,  51, 495, 327,  50, 196, 117, 264, 206,\n",
      "         73, 221, 116, 357, 249, 311, 452,  27,  10,  81, 113,  20,  96, 370,\n",
      "        355, 238, 405, 298, 348, 497, 441, 130, 297,  82, 345, 190, 267, 265,\n",
      "        391, 289, 425, 135,  64, 470, 136, 212, 274, 213, 344, 149, 204, 476,\n",
      "        453, 247, 398,  77,  25, 142, 185, 231, 308, 399, 339, 143, 456, 158,\n",
      "        121,  36, 243, 435, 508, 163, 224, 429, 324, 396, 323, 229,  43, 128,\n",
      "        404, 172, 202,  95, 373, 350, 192, 268, 503, 256, 509, 383, 413, 335,\n",
      "        471, 351, 489, 427, 138, 329, 465, 148, 248,  24, 126,  74, 207, 261,\n",
      "         46, 188,  88, 175, 432, 296, 179, 134, 122, 439, 343, 286, 254, 132,\n",
      "        362, 387, 451, 171, 319, 232, 494, 109, 123, 101, 389, 369, 301, 367,\n",
      "        200, 266, 290, 438,  23, 469, 455,  56, 118, 197, 299, 474, 277, 492,\n",
      "         39, 285,  55, 198, 420, 287, 421, 209, 359, 189,  11, 410,  37, 466,\n",
      "        390,  93, 419, 178, 228, 191, 406,  98])\n",
      "saving_filter_idices : tensor([ 31, 487, 352, 341, 463, 219, 222, 349, 281, 333, 210,  91, 485, 457,\n",
      "        180, 504, 488, 490, 304, 273, 125, 407, 310, 181, 115,  62,  35, 346,\n",
      "        356,  58, 393, 255,  89, 230, 388, 315, 340, 371, 211, 144, 491, 173,\n",
      "        338,  65, 259, 110, 473, 445, 505, 216, 309,  67, 114, 450,  45, 108,\n",
      "        155, 102, 409, 111,  94, 193, 332, 119, 166, 479, 447,  12, 510, 454,\n",
      "        401, 467, 227, 325, 337, 402,  72, 321, 250, 462, 152, 184, 376,  79,\n",
      "         13, 223, 201, 433, 392,  42, 291, 233, 300,  32, 444, 334, 244, 411,\n",
      "        252, 292,  33, 448, 501, 133,  57, 169,  18, 288,  61, 164, 353, 449,\n",
      "        139,  86,  75, 317,  26,  92,  52, 422, 145,   2, 400,   3, 498, 270,\n",
      "         21, 100, 484, 283, 205, 239, 280, 182, 241,  70, 418, 103,  69, 363,\n",
      "        378, 295,  15,  28, 499, 303, 104, 129,  41, 446, 458,   5,  30,  80,\n",
      "        368, 472, 394, 354,  59,  34, 386, 176, 195, 137, 150,  63, 260, 156,\n",
      "        263, 269, 477, 414,   1,  19, 272, 500, 330, 360, 170, 361, 461, 397,\n",
      "        496, 347, 157, 282, 459, 358, 415, 294,  90, 511, 440,  76, 236, 194,\n",
      "        159, 326, 417, 151, 482,  78, 437, 168, 377, 271, 218, 208, 140,   0,\n",
      "        486,  17,   8,  71, 234, 480, 105, 187,  40, 342,  87, 174, 217, 225,\n",
      "        320, 245, 384, 165, 328, 302, 395, 147, 426, 162, 443, 365, 478, 307,\n",
      "        372, 483, 257, 493, 203, 154, 183, 316, 423,  16, 258,   4, 364, 153,\n",
      "        177, 318,  22, 235])\n",
      "pruned_weight.shape : torch.Size([256, 256, 3, 3])\n",
      "pruned_bias.shape : torch.Size([256])\n",
      "pruned_bn_gamma.shape : torch.Size([256])\n",
      "pruned_bn_beta.shape : torch.Size([256])\n",
      "pruned_bn_running_mean.shape : torch.Size([256])\n",
      "pruned_bn_running_var.shape : torch.Size([256])\n",
      "pruned_next_weight.shape : torch.Size([512, 256, 3, 3])\n",
      "========================================  conv12  ========================================\n",
      "pruned_rate : 50%\n",
      "num_prune_channels : 256\n",
      "weight.shape : torch.Size([512, 256, 3, 3])\n",
      "bias.shape : torch.Size([512])\n",
      "bn_gamma.shape : torch.Size([512])\n",
      "bn_beta.shape : torch.Size([512])\n",
      "bn_running_mean.shape : torch.Size([512])\n",
      "bn_running_var.shape : torch.Size([512])\n",
      "sorted_weight_indices : tensor([186, 212, 415, 127, 337, 216,  34, 204, 329, 124, 417,  18,  85, 301,\n",
      "        486, 311, 140, 151, 430, 278,  21, 281, 422, 215, 257,  73,  11, 226,\n",
      "         41, 390, 236, 392,   1, 451, 493,  99, 178, 462, 489, 461, 143, 120,\n",
      "        361,  33, 313, 310, 262, 360, 162, 457, 166,  86, 189,  39,  32, 149,\n",
      "        375, 250, 495, 378, 243, 290, 442, 110, 150, 188, 259, 466, 485, 347,\n",
      "        240, 441, 279, 292, 320, 341, 148, 437, 405, 187,  71, 233, 427, 274,\n",
      "        228, 295, 287, 272,  12,  88, 368, 482,  57, 414, 129, 454, 455, 404,\n",
      "         97, 491,  23, 475, 334,  91, 179, 502, 465,   7, 460, 367, 235,  64,\n",
      "        201, 339, 432, 471, 397, 356, 118, 115, 229,  50, 199, 431, 492,  13,\n",
      "         16, 217, 401, 470, 231, 142, 138,  25, 197,  77, 219, 315, 255, 284,\n",
      "        205, 407, 109, 155, 456, 254,  60, 453, 137, 506,   9, 141, 102,  45,\n",
      "        171, 280, 445, 300, 319,  65, 220, 385, 261, 403,   3, 111,  67, 363,\n",
      "        429, 207, 154,  62, 123,  66, 291, 338, 387, 308, 478,  58, 497, 185,\n",
      "        101, 406, 440, 299, 359,  90,  31,  84,  10, 366, 181, 443,  68, 293,\n",
      "        114, 246, 364, 132,  59, 324, 434, 439,  75, 416,  46, 241, 372, 484,\n",
      "        224, 323, 294, 413, 373,  89, 222, 265, 159, 164, 419, 473, 161, 198,\n",
      "         44, 175,  37, 157,  47, 176, 418, 230, 389, 336, 400, 245, 158, 275,\n",
      "        452, 340, 238, 104,   8,  93, 370, 160, 139, 507, 322, 330, 208, 438,\n",
      "        349, 184, 435, 283, 221, 380, 411, 247, 344, 249, 169, 119,  26,  98,\n",
      "        487, 511, 227,  17, 136, 383, 307, 244,  69,   6, 193, 173,  61, 153,\n",
      "        125, 355, 182, 266, 146, 191, 234, 145, 225, 357,  95, 377, 472, 386,\n",
      "        256,  42,  72, 499,  83, 444, 270, 297, 206,  22, 211,  81, 333, 483,\n",
      "        152, 252, 218, 258,   5, 458, 277, 488, 100, 303, 106, 242, 494, 112,\n",
      "        223, 412, 463, 384, 180,  78,   4, 163, 381, 285, 331,  15,  96, 167,\n",
      "        209,  43, 468, 467, 202, 328, 128, 421,  38, 122, 260,  76, 353, 479,\n",
      "         19, 388, 190,  55,  49, 423,  24, 410, 203, 374, 424, 312, 309, 459,\n",
      "         36, 251, 379, 409, 117, 144, 490, 196, 177, 126, 183,  82, 170, 505,\n",
      "        504,  35, 469, 288, 343, 302, 113, 269, 498, 200,  54,  53, 448, 436,\n",
      "        289,  48,  51,  14,  70, 351, 450, 425, 369, 116, 248,  40, 480, 232,\n",
      "        335, 447, 477, 321, 348,  94, 428,  29, 396, 133, 213, 306, 394, 108,\n",
      "        318,  63, 264,  74, 391, 253, 476,  28, 408, 474, 276,  20,  27, 210,\n",
      "         92, 135, 134, 501, 345, 147, 395, 105, 481,  79, 286, 371, 107, 267,\n",
      "        508, 263, 130, 342, 172, 305, 350, 352, 121, 194, 165, 317, 237, 382,\n",
      "        316, 420, 156, 325, 103, 282, 398, 271,   2,   0,  87, 346,  56, 354,\n",
      "        314,  80, 376, 362, 365, 168, 433, 332, 503, 239, 510, 174, 358, 509,\n",
      "        393, 192,  52, 449, 327, 273, 131, 496, 195, 402, 326, 399, 464, 446,\n",
      "        296, 500, 304, 268, 214, 426,  30, 298])\n",
      "saving_filter_idices : tensor([186, 212, 415, 127, 337, 216,  34, 204, 329, 124, 417,  18,  85, 301,\n",
      "        486, 311, 140, 151, 430, 278,  21, 281, 422, 215, 257,  73,  11, 226,\n",
      "         41, 390, 236, 392,   1, 451, 493,  99, 178, 462, 489, 461, 143, 120,\n",
      "        361,  33, 313, 310, 262, 360, 162, 457, 166,  86, 189,  39,  32, 149,\n",
      "        375, 250, 495, 378, 243, 290, 442, 110, 150, 188, 259, 466, 485, 347,\n",
      "        240, 441, 279, 292, 320, 341, 148, 437, 405, 187,  71, 233, 427, 274,\n",
      "        228, 295, 287, 272,  12,  88, 368, 482,  57, 414, 129, 454, 455, 404,\n",
      "         97, 491,  23, 475, 334,  91, 179, 502, 465,   7, 460, 367, 235,  64,\n",
      "        201, 339, 432, 471, 397, 356, 118, 115, 229,  50, 199, 431, 492,  13,\n",
      "         16, 217, 401, 470, 231, 142, 138,  25, 197,  77, 219, 315, 255, 284,\n",
      "        205, 407, 109, 155, 456, 254,  60, 453, 137, 506,   9, 141, 102,  45,\n",
      "        171, 280, 445, 300, 319,  65, 220, 385, 261, 403,   3, 111,  67, 363,\n",
      "        429, 207, 154,  62, 123,  66, 291, 338, 387, 308, 478,  58, 497, 185,\n",
      "        101, 406, 440, 299, 359,  90,  31,  84,  10, 366, 181, 443,  68, 293,\n",
      "        114, 246, 364, 132,  59, 324, 434, 439,  75, 416,  46, 241, 372, 484,\n",
      "        224, 323, 294, 413, 373,  89, 222, 265, 159, 164, 419, 473, 161, 198,\n",
      "         44, 175,  37, 157,  47, 176, 418, 230, 389, 336, 400, 245, 158, 275,\n",
      "        452, 340, 238, 104,   8,  93, 370, 160, 139, 507, 322, 330, 208, 438,\n",
      "        349, 184, 435, 283])\n",
      "pruned_weight.shape : torch.Size([256, 256, 3, 3])\n",
      "pruned_bias.shape : torch.Size([256])\n",
      "pruned_bn_gamma.shape : torch.Size([256])\n",
      "pruned_bn_beta.shape : torch.Size([256])\n",
      "pruned_bn_running_mean.shape : torch.Size([256])\n",
      "pruned_bn_running_var.shape : torch.Size([256])\n",
      "pruned_next_weight.shape : torch.Size([512, 256, 3, 3])\n",
      "========================================  conv13  ========================================\n",
      "pruned_rate : 50%\n",
      "num_prune_channels : 256\n",
      "weight.shape : torch.Size([512, 256, 3, 3])\n",
      "bias.shape : torch.Size([512])\n",
      "bn_gamma.shape : torch.Size([512])\n",
      "bn_beta.shape : torch.Size([512])\n",
      "bn_running_mean.shape : torch.Size([512])\n",
      "bn_running_var.shape : torch.Size([512])\n",
      "sorted_weight_indices : tensor([ 66, 487, 253, 474, 173, 459, 420, 316, 427, 122, 245, 501, 423, 396,\n",
      "        287, 461, 284, 443, 215, 274,  57, 382, 105,  35, 213, 128, 323, 208,\n",
      "        457, 220,  42, 417, 505, 307, 426, 492, 293,  95,  75,  12,   7, 268,\n",
      "        269, 116, 418, 187, 171, 295, 350, 164, 134, 503, 178,  94,  61, 508,\n",
      "        176, 175,   1, 490, 344, 211,  29, 504, 413, 237, 242, 446, 193, 467,\n",
      "        231, 357, 159, 345, 225, 251, 331,   6, 202, 272, 191, 190, 469, 303,\n",
      "          0, 374, 153, 373,   8, 341, 294, 440,  80, 172, 221, 433, 466,  46,\n",
      "        397, 458, 169, 195, 486, 318, 224, 300, 361, 115,  32,  70, 267, 181,\n",
      "        338,  77, 297, 174, 230, 136,  48, 445, 346, 188,  73, 488,  17, 358,\n",
      "        305, 448, 147, 309, 271, 210, 451, 257, 157, 336, 118, 365, 114, 259,\n",
      "        260, 110, 247, 298,  18, 258, 332, 416, 185, 104, 262, 143,  93, 108,\n",
      "        509,  68, 380, 301, 206,  16,  20, 200, 184, 113, 449, 125, 186, 444,\n",
      "        222, 312, 478,  82, 204, 310, 241, 236, 372, 348, 390, 250, 383, 453,\n",
      "        438, 133, 235, 263, 377,  97, 325, 408, 150, 362,  69, 386, 388,  30,\n",
      "        227,  63, 214, 144, 161,  98, 101,  96, 100,  22,  54, 349, 228, 411,\n",
      "        140, 352, 146, 240, 470,   2, 319, 442, 351,  49, 479, 354, 112, 126,\n",
      "        491, 137,  39, 340, 314, 264, 378, 290, 138, 244, 367, 454, 256, 320,\n",
      "        477, 324,  38,  99,  21, 177,   3, 510, 103, 154, 189,  89, 401,   4,\n",
      "        485, 387, 364, 385, 476, 207, 130,   5, 201, 450, 432,  43,  28, 480,\n",
      "        111, 135, 302, 249,  25, 180, 498,  88,  44, 399,  41, 167, 455, 462,\n",
      "        311, 166, 313, 391, 304, 109, 306, 238, 493, 484, 428, 506, 131,  79,\n",
      "        406, 218, 276, 192, 410, 468,  65, 155, 205, 255, 179, 363, 429, 482,\n",
      "         34, 286, 376, 229, 296, 124, 392, 369,  53, 212, 330, 152, 424, 254,\n",
      "        347, 475, 473, 360, 464, 142,  58, 288,  40,  85,  91, 299,  60, 132,\n",
      "        499, 434,  27, 123,  19, 343, 148, 430, 414, 270, 209, 170, 342, 106,\n",
      "        217, 226,  24, 415, 317, 489, 203, 165, 246,  15, 327,  67,  59,  84,\n",
      "         62, 239, 436, 266,  90, 291, 398, 405, 460, 280, 145,  50, 315, 162,\n",
      "        234,  83, 281, 232, 400, 409, 356, 265,  78, 107, 275, 404, 261, 289,\n",
      "        368,  92, 129, 273, 393, 447, 419,  86, 322, 496, 370, 472, 335, 102,\n",
      "         51, 495, 497,  55, 197, 196, 326, 321, 507, 494, 375, 463, 483, 389,\n",
      "        199,  74,  76,  45, 277, 119, 334, 127, 441,  31,  37,  13, 452, 425,\n",
      "         81, 151,  47, 285, 158,  71, 366, 359,  56,  52, 219, 355,  26, 243,\n",
      "        403, 333, 471, 248, 394, 395,  36, 337, 371, 465, 328, 198, 435,  72,\n",
      "        223, 183, 279, 278, 233, 168, 384,  87, 379, 431, 422, 381, 439, 252,\n",
      "        339, 481,   9,  10, 182, 194, 308, 402, 353, 292, 283, 156, 407, 282,\n",
      "        139, 160,  14, 141, 502, 500, 329, 120,  33,  11,  64,  23, 412, 149,\n",
      "        163, 511, 121, 216, 117, 421, 437, 456])\n",
      "saving_filter_idices : tensor([ 66, 487, 253, 474, 173, 459, 420, 316, 427, 122, 245, 501, 423, 396,\n",
      "        287, 461, 284, 443, 215, 274,  57, 382, 105,  35, 213, 128, 323, 208,\n",
      "        457, 220,  42, 417, 505, 307, 426, 492, 293,  95,  75,  12,   7, 268,\n",
      "        269, 116, 418, 187, 171, 295, 350, 164, 134, 503, 178,  94,  61, 508,\n",
      "        176, 175,   1, 490, 344, 211,  29, 504, 413, 237, 242, 446, 193, 467,\n",
      "        231, 357, 159, 345, 225, 251, 331,   6, 202, 272, 191, 190, 469, 303,\n",
      "          0, 374, 153, 373,   8, 341, 294, 440,  80, 172, 221, 433, 466,  46,\n",
      "        397, 458, 169, 195, 486, 318, 224, 300, 361, 115,  32,  70, 267, 181,\n",
      "        338,  77, 297, 174, 230, 136,  48, 445, 346, 188,  73, 488,  17, 358,\n",
      "        305, 448, 147, 309, 271, 210, 451, 257, 157, 336, 118, 365, 114, 259,\n",
      "        260, 110, 247, 298,  18, 258, 332, 416, 185, 104, 262, 143,  93, 108,\n",
      "        509,  68, 380, 301, 206,  16,  20, 200, 184, 113, 449, 125, 186, 444,\n",
      "        222, 312, 478,  82, 204, 310, 241, 236, 372, 348, 390, 250, 383, 453,\n",
      "        438, 133, 235, 263, 377,  97, 325, 408, 150, 362,  69, 386, 388,  30,\n",
      "        227,  63, 214, 144, 161,  98, 101,  96, 100,  22,  54, 349, 228, 411,\n",
      "        140, 352, 146, 240, 470,   2, 319, 442, 351,  49, 479, 354, 112, 126,\n",
      "        491, 137,  39, 340, 314, 264, 378, 290, 138, 244, 367, 454, 256, 320,\n",
      "        477, 324,  38,  99,  21, 177,   3, 510, 103, 154, 189,  89, 401,   4,\n",
      "        485, 387, 364, 385])\n",
      "pruned_weight.shape : torch.Size([256, 256, 3, 3])\n",
      "pruned_bias.shape : torch.Size([256])\n",
      "pruned_bn_gamma.shape : torch.Size([256])\n",
      "pruned_bn_beta.shape : torch.Size([256])\n",
      "pruned_bn_running_mean.shape : torch.Size([256])\n",
      "pruned_bn_running_var.shape : torch.Size([256])\n",
      "pruned_next_weight.shape : torch.Size([512, 256])\n"
     ]
    }
   ],
   "source": [
    "new_pruned_model = getOneShotPrunedModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "      Layer (type)          Input Shape         Param #     Tr. Param #\n",
      "========================================================================\n",
      "          Conv2d-1       [1, 3, 32, 32]             896             896\n",
      "     BatchNorm2d-2      [1, 32, 32, 32]              64              64\n",
      "            ReLU-3      [1, 32, 32, 32]               0               0\n",
      "          Conv2d-4      [1, 32, 32, 32]          18,496          18,496\n",
      "     BatchNorm2d-5      [1, 64, 32, 32]             128             128\n",
      "            ReLU-6      [1, 64, 32, 32]               0               0\n",
      "       MaxPool2d-7      [1, 64, 32, 32]               0               0\n",
      "          Conv2d-8      [1, 64, 16, 16]          73,856          73,856\n",
      "     BatchNorm2d-9     [1, 128, 16, 16]             256             256\n",
      "           ReLU-10     [1, 128, 16, 16]               0               0\n",
      "         Conv2d-11     [1, 128, 16, 16]         147,584         147,584\n",
      "    BatchNorm2d-12     [1, 128, 16, 16]             256             256\n",
      "           ReLU-13     [1, 128, 16, 16]               0               0\n",
      "      MaxPool2d-14     [1, 128, 16, 16]               0               0\n",
      "         Conv2d-15       [1, 128, 8, 8]         295,168         295,168\n",
      "    BatchNorm2d-16       [1, 256, 8, 8]             512             512\n",
      "           ReLU-17       [1, 256, 8, 8]               0               0\n",
      "         Conv2d-18       [1, 256, 8, 8]         590,080         590,080\n",
      "    BatchNorm2d-19       [1, 256, 8, 8]             512             512\n",
      "           ReLU-20       [1, 256, 8, 8]               0               0\n",
      "         Conv2d-21       [1, 256, 8, 8]         590,080         590,080\n",
      "    BatchNorm2d-22       [1, 256, 8, 8]             512             512\n",
      "           ReLU-23       [1, 256, 8, 8]               0               0\n",
      "      MaxPool2d-24       [1, 256, 8, 8]               0               0\n",
      "         Conv2d-25       [1, 256, 4, 4]         590,080         590,080\n",
      "    BatchNorm2d-26       [1, 256, 4, 4]             512             512\n",
      "           ReLU-27       [1, 256, 4, 4]               0               0\n",
      "         Conv2d-28       [1, 256, 4, 4]         590,080         590,080\n",
      "    BatchNorm2d-29       [1, 256, 4, 4]             512             512\n",
      "           ReLU-30       [1, 256, 4, 4]               0               0\n",
      "         Conv2d-31       [1, 256, 4, 4]         590,080         590,080\n",
      "    BatchNorm2d-32       [1, 256, 4, 4]             512             512\n",
      "           ReLU-33       [1, 256, 4, 4]               0               0\n",
      "      MaxPool2d-34       [1, 256, 4, 4]               0               0\n",
      "         Conv2d-35       [1, 256, 2, 2]         590,080         590,080\n",
      "    BatchNorm2d-36       [1, 256, 2, 2]             512             512\n",
      "           ReLU-37       [1, 256, 2, 2]               0               0\n",
      "         Conv2d-38       [1, 256, 2, 2]         590,080         590,080\n",
      "    BatchNorm2d-39       [1, 256, 2, 2]             512             512\n",
      "           ReLU-40       [1, 256, 2, 2]               0               0\n",
      "         Conv2d-41       [1, 256, 2, 2]         590,080         590,080\n",
      "    BatchNorm2d-42       [1, 256, 2, 2]             512             512\n",
      "           ReLU-43       [1, 256, 2, 2]               0               0\n",
      "      MaxPool2d-44       [1, 256, 2, 2]               0               0\n",
      "        Flatten-45       [1, 256, 1, 1]               0               0\n",
      "         Linear-46             [1, 256]         131,584         131,584\n",
      "    BatchNorm1d-47             [1, 512]           1,024           1,024\n",
      "           ReLU-48             [1, 512]               0               0\n",
      "         Linear-49             [1, 512]           5,130           5,130\n",
      "========================================================================\n",
      "Total params: 5,399,690\n",
      "Trainable params: 5,399,690\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "new_pruned_model.to(device)\n",
    "showNewPrunedModel(new_pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Accuracy, #FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "# test datas : 10000\n",
      "Top1 Accuracy : 91.47, Top5 Accuracy : 99.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loader, tesize = loadValDataset()\n",
    "\n",
    "# load best model\n",
    "model = VGG16_BN_PRUNE_FOR_SCRATCH()\n",
    "checkpoint = torch.load('checkpoint/best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "top1_acc, top5_acc = testAccuracy(model, val_loader)\n",
    "print(f\"Top1 Accuracy : {top1_acc:.2f}, Top5 Accuracy : {top5_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters : 5399690\n"
     ]
    }
   ],
   "source": [
    "# check the number of parameters\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of Parameters : {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        17.93%     899.000us        88.24%       4.425ms       4.425ms       0.000us         0.00%     328.000us     328.000us             1  \n",
      "                                      aten::convolution         1.66%      83.000us        43.35%       2.174ms     167.231us       0.000us         0.00%     270.000us      20.769us            13  \n",
      "                                     aten::_convolution         1.71%      86.000us        41.69%       2.091ms     160.846us       0.000us         0.00%     270.000us      20.769us            13  \n",
      "                                           aten::conv2d         2.29%     115.000us        43.69%       2.191ms     168.538us       0.000us         0.00%     252.000us      19.385us            13  \n",
      "                                aten::cudnn_convolution        32.50%       1.630ms        36.53%       1.832ms     140.923us     226.000us        75.59%     237.000us      18.231us            13  \n",
      "sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     130.000us        43.48%     130.000us      11.818us            11  \n",
      "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us      80.000us        26.76%      80.000us       3.333us            24  \n",
      "                                             aten::add_         1.97%      99.000us         2.89%     145.000us      11.154us      20.000us         6.69%      33.000us       2.538us            13  \n",
      "                           aten::_batch_norm_impl_index         0.64%      32.000us        12.68%     636.000us      45.429us       0.000us         0.00%      28.000us       2.000us            14  \n",
      "                                 aten::cudnn_batch_norm         5.80%     291.000us        10.19%     511.000us      39.308us      20.000us         6.69%      25.000us       1.923us            13  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 5.015ms\n",
      "Self CUDA time total: 299.000us\n",
      "\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::cudnn_convolution        32.50%       1.630ms        36.53%       1.832ms     140.923us     226.000us        75.59%     237.000us      18.231us            13  \n",
      "                                        model_inference        17.93%     899.000us        88.24%       4.425ms       4.425ms       0.000us         0.00%     328.000us     328.000us             1  \n",
      "                                  cudaDeviceSynchronize        11.76%     590.000us        11.76%     590.000us     590.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel         6.94%     348.000us         6.94%     348.000us       4.703us      13.000us         4.35%      13.000us       0.176us            74  \n",
      "                                 aten::cudnn_batch_norm         5.80%     291.000us        10.19%     511.000us      39.308us      20.000us         6.69%      25.000us       1.923us            13  \n",
      "                                            aten::empty         2.93%     147.000us         2.93%     147.000us       2.673us       0.000us         0.00%       0.000us       0.000us            55  \n",
      "                                       aten::batch_norm         2.59%     130.000us        12.96%     650.000us      46.429us       0.000us         0.00%      24.000us       1.714us            14  \n",
      "                                           aten::conv2d         2.29%     115.000us        43.69%       2.191ms     168.538us       0.000us         0.00%     252.000us      19.385us            13  \n",
      "                                            aten::relu_         2.15%     108.000us         4.87%     244.000us      17.429us       0.000us         0.00%      14.000us       1.000us            14  \n",
      "                                             aten::add_         1.97%      99.000us         2.89%     145.000us      11.154us      20.000us         6.69%      33.000us       2.538us            13  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 5.015ms\n",
      "Self CUDA time total: 299.000us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-06 16:51:24 2771716:2771716 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-02-06 16:51:24 2771716:2771716 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-02-06 16:51:24 2771716:2771716 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch\n",
    "\n",
    "# Define your model and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Sample input data\n",
    "input_data = torch.randn(1, 3, 32, 32).to(device)\n",
    "\n",
    "# Profiler를 사용하여 FLOPS 계산\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(input_data)\n",
    "        \n",
    "# Print the profile results\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "# flops\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of flops before pruning\n",
      "Op Flatten is not supported at now, set FLOPs of it to zero.\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| module_name   | module_type   | in_shape    | out_shape   | kernel_size,padding,stride   | params   | params_percent   | params_percent_vis   | flops    | flops_percent   | flops_percent_vis   |\n",
      "+===============+===============+=============+=============+==============================+==========+==================+======================+==========+=================+=====================+\n",
      "| conv1.0       | Conv2d        | (3,32,32)   | (64,32,32)  | k=(3, 3), p=(1, 1), s=(1, 1) | 1.792K   | 0.0119531%       |                      | 1.83501M | 0.583105%       |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv1.1       | BatchNorm2d   | (64,32,32)  | (64,32,32)  |                              | 128.0    | 0.000853792%     |                      | 131.072K | 0.0416503%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv1.2       | ReLU          | (64,32,32)  | (64,32,32)  |                              | 0.0      | 0.0%             |                      | 65.536K  | 0.0208252%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv2.0       | Conv2d        | (64,32,32)  | (64,32,32)  | k=(3, 3), p=(1, 1), s=(1, 1) | 36.928K  | 0.246319%        |                      | 37.8143M | 12.0161%        | ######              |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv2.1       | BatchNorm2d   | (64,32,32)  | (64,32,32)  |                              | 128.0    | 0.000853792%     |                      | 131.072K | 0.0416503%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv2.2       | ReLU          | (64,32,32)  | (64,32,32)  |                              | 0.0      | 0.0%             |                      | 65.536K  | 0.0208252%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv2.3       | MaxPool2d     | (64,32,32)  | (64,16,16)  |                              | 0.0      | 0.0%             |                      | 65.536K  | 0.0208252%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv3.0       | Conv2d        | (64,16,16)  | (128,16,16) | k=(3, 3), p=(1, 1), s=(1, 1) | 73.856K  | 0.492638%        |                      | 18.9071M | 6.00806%        | ###                 |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv3.1       | BatchNorm2d   | (128,16,16) | (128,16,16) |                              | 256.0    | 0.00170758%      |                      | 65.536K  | 0.0208252%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv3.2       | ReLU          | (128,16,16) | (128,16,16) |                              | 0.0      | 0.0%             |                      | 32.768K  | 0.0104126%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv4.0       | Conv2d        | (128,16,16) | (128,16,16) | k=(3, 3), p=(1, 1), s=(1, 1) | 147.584K | 0.984422%        |                      | 37.7815M | 12.0057%        | ######              |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv4.1       | BatchNorm2d   | (128,16,16) | (128,16,16) |                              | 256.0    | 0.00170758%      |                      | 65.536K  | 0.0208252%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv4.2       | ReLU          | (128,16,16) | (128,16,16) |                              | 0.0      | 0.0%             |                      | 32.768K  | 0.0104126%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv4.3       | MaxPool2d     | (128,16,16) | (128,8,8)   |                              | 0.0      | 0.0%             |                      | 32.768K  | 0.0104126%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv5.0       | Conv2d        | (128,8,8)   | (256,8,8)   | k=(3, 3), p=(1, 1), s=(1, 1) | 295.168K | 1.96884%         |                      | 18.8908M | 6.00285%        | ###                 |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv5.1       | BatchNorm2d   | (256,8,8)   | (256,8,8)   |                              | 512.0    | 0.00341517%      |                      | 32.768K  | 0.0104126%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv5.2       | ReLU          | (256,8,8)   | (256,8,8)   |                              | 0.0      | 0.0%             |                      | 16.384K  | 0.00520629%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv6.0       | Conv2d        | (256,8,8)   | (256,8,8)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 3.93598%         | #                    | 37.7651M | 12.0005%        | ######              |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv6.1       | BatchNorm2d   | (256,8,8)   | (256,8,8)   |                              | 512.0    | 0.00341517%      |                      | 32.768K  | 0.0104126%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv6.2       | ReLU          | (256,8,8)   | (256,8,8)   |                              | 0.0      | 0.0%             |                      | 16.384K  | 0.00520629%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv7.0       | Conv2d        | (256,8,8)   | (256,8,8)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 3.93598%         | #                    | 37.7651M | 12.0005%        | ######              |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv7.1       | BatchNorm2d   | (256,8,8)   | (256,8,8)   |                              | 512.0    | 0.00341517%      |                      | 32.768K  | 0.0104126%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv7.2       | ReLU          | (256,8,8)   | (256,8,8)   |                              | 0.0      | 0.0%             |                      | 16.384K  | 0.00520629%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv7.3       | MaxPool2d     | (256,8,8)   | (256,4,4)   |                              | 0.0      | 0.0%             |                      | 16.384K  | 0.00520629%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv8.0       | Conv2d        | (256,4,4)   | (512,4,4)   | k=(3, 3), p=(1, 1), s=(1, 1) | 1.18016M | 7.87196%         | ###                  | 18.8826M | 6.00025%        | ###                 |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv8.1       | BatchNorm2d   | (512,4,4)   | (512,4,4)   |                              | 1.024K   | 0.00683033%      |                      | 16.384K  | 0.00520629%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv8.2       | ReLU          | (512,4,4)   | (512,4,4)   |                              | 0.0      | 0.0%             |                      | 8.192K   | 0.00260315%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv9.0       | Conv2d        | (512,4,4)   | (512,4,4)   | k=(3, 3), p=(1, 1), s=(1, 1) | 2.35981M | 15.7405%         | #######              | 37.7569M | 11.9979%        | #####               |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv9.1       | BatchNorm2d   | (512,4,4)   | (512,4,4)   |                              | 1.024K   | 0.00683033%      |                      | 16.384K  | 0.00520629%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv9.2       | ReLU          | (512,4,4)   | (512,4,4)   |                              | 0.0      | 0.0%             |                      | 8.192K   | 0.00260315%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv10.0      | Conv2d        | (512,4,4)   | (512,4,4)   | k=(3, 3), p=(1, 1), s=(1, 1) | 2.35981M | 15.7405%         | #######              | 37.7569M | 11.9979%        | #####               |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv10.1      | BatchNorm2d   | (512,4,4)   | (512,4,4)   |                              | 1.024K   | 0.00683033%      |                      | 16.384K  | 0.00520629%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv10.2      | ReLU          | (512,4,4)   | (512,4,4)   |                              | 0.0      | 0.0%             |                      | 8.192K   | 0.00260315%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv10.3      | MaxPool2d     | (512,4,4)   | (512,2,2)   |                              | 0.0      | 0.0%             |                      | 8.192K   | 0.00260315%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv11.0      | Conv2d        | (512,2,2)   | (512,2,2)   | k=(3, 3), p=(1, 1), s=(1, 1) | 2.35981M | 15.7405%         | #######              | 9.43923M | 2.99947%        | #                   |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv11.1      | BatchNorm2d   | (512,2,2)   | (512,2,2)   |                              | 1.024K   | 0.00683033%      |                      | 4.096K   | 0.00130157%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv11.2      | ReLU          | (512,2,2)   | (512,2,2)   |                              | 0.0      | 0.0%             |                      | 2.048K   | 0.000650786%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv12.0      | Conv2d        | (512,2,2)   | (512,2,2)   | k=(3, 3), p=(1, 1), s=(1, 1) | 2.35981M | 15.7405%         | #######              | 9.43923M | 2.99947%        | #                   |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv12.1      | BatchNorm2d   | (512,2,2)   | (512,2,2)   |                              | 1.024K   | 0.00683033%      |                      | 4.096K   | 0.00130157%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv12.2      | ReLU          | (512,2,2)   | (512,2,2)   |                              | 0.0      | 0.0%             |                      | 2.048K   | 0.000650786%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv13.0      | Conv2d        | (512,2,2)   | (512,2,2)   | k=(3, 3), p=(1, 1), s=(1, 1) | 2.35981M | 15.7405%         | #######              | 9.43923M | 2.99947%        | #                   |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv13.1      | BatchNorm2d   | (512,2,2)   | (512,2,2)   |                              | 1.024K   | 0.00683033%      |                      | 4.096K   | 0.00130157%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv13.2      | ReLU          | (512,2,2)   | (512,2,2)   |                              | 0.0      | 0.0%             |                      | 2.048K   | 0.000650786%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv13.3      | MaxPool2d     | (512,2,2)   | (512,1,1)   |                              | 0.0      | 0.0%             |                      | 2.048K   | 0.000650786%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc1.0         | Flatten       | (512,1,1)   | (512)       |                              | 0.0      | 0.0%             |                      | 0.0      | 0.0%            |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc1.1         | Linear        | (512)       | (512)       |                              | 262.656K | 1.75198%         |                      | 262.144K | 0.0833007%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc1.2         | BatchNorm1d   | (512)       | (512)       |                              | 1.024K   | 0.00683033%      |                      | 1.024K   | 0.000325393%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc1.3         | ReLU          | (512)       | (512)       |                              | 0.0      | 0.0%             |                      | 512.0    | 0.000162697%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc2.0         | Linear        | (512)       | (10)        |                              | 5.13K    | 0.0342184%       |                      | 5.12K    | 0.00162697%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "\n",
      "\n",
      "FLOPs: 314.696M\n",
      "Params: 14.9919M\n"
     ]
    }
   ],
   "source": [
    "print(\"the number of flops before pruning\")\n",
    "! flopth -m VGG16_BN -p ../architecture -i 3 32 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of flops after pruning\n",
      "Op Flatten is not supported at now, set FLOPs of it to zero.\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| module_name   | module_type   | in_shape    | out_shape   | kernel_size,padding,stride   | params   | params_percent   | params_percent_vis   | flops    | flops_percent   | flops_percent_vis   |\n",
      "+===============+===============+=============+=============+==============================+==========+==================+======================+==========+=================+=====================+\n",
      "| conv1.0       | Conv2d        | (3,32,32)   | (32,32,32)  | k=(3, 3), p=(1, 1), s=(1, 1) | 896.0    | 0.0165935%       |                      | 917.504K | 0.442566%       |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv1.1       | BatchNorm2d   | (32,32,32)  | (32,32,32)  |                              | 64.0     | 0.00118525%      |                      | 65.536K  | 0.0316119%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv1.2       | ReLU          | (32,32,32)  | (32,32,32)  |                              | 0.0      | 0.0%             |                      | 32.768K  | 0.0158059%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv2.0       | Conv2d        | (32,32,32)  | (64,32,32)  | k=(3, 3), p=(1, 1), s=(1, 1) | 18.496K  | 0.342538%        |                      | 18.9399M | 9.13583%        | ####                |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv2.1       | BatchNorm2d   | (64,32,32)  | (64,32,32)  |                              | 128.0    | 0.00237051%      |                      | 131.072K | 0.0632238%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv2.2       | ReLU          | (64,32,32)  | (64,32,32)  |                              | 0.0      | 0.0%             |                      | 65.536K  | 0.0316119%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv2.3       | MaxPool2d     | (64,32,32)  | (64,16,16)  |                              | 0.0      | 0.0%             |                      | 65.536K  | 0.0316119%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv3.0       | Conv2d        | (64,16,16)  | (128,16,16) | k=(3, 3), p=(1, 1), s=(1, 1) | 73.856K  | 1.36778%         |                      | 18.9071M | 9.12003%        | ####                |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv3.1       | BatchNorm2d   | (128,16,16) | (128,16,16) |                              | 256.0    | 0.00474101%      |                      | 65.536K  | 0.0316119%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv3.2       | ReLU          | (128,16,16) | (128,16,16) |                              | 0.0      | 0.0%             |                      | 32.768K  | 0.0158059%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv4.0       | Conv2d        | (128,16,16) | (128,16,16) | k=(3, 3), p=(1, 1), s=(1, 1) | 147.584K | 2.73319%         | #                    | 37.7815M | 18.2243%        | #########           |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv4.1       | BatchNorm2d   | (128,16,16) | (128,16,16) |                              | 256.0    | 0.00474101%      |                      | 65.536K  | 0.0316119%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv4.2       | ReLU          | (128,16,16) | (128,16,16) |                              | 0.0      | 0.0%             |                      | 32.768K  | 0.0158059%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv4.3       | MaxPool2d     | (128,16,16) | (128,8,8)   |                              | 0.0      | 0.0%             |                      | 32.768K  | 0.0158059%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv5.0       | Conv2d        | (128,8,8)   | (256,8,8)   | k=(3, 3), p=(1, 1), s=(1, 1) | 295.168K | 5.46639%         | ##                   | 18.8908M | 9.11213%        | ####                |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv5.1       | BatchNorm2d   | (256,8,8)   | (256,8,8)   |                              | 512.0    | 0.00948203%      |                      | 32.768K  | 0.0158059%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv5.2       | ReLU          | (256,8,8)   | (256,8,8)   |                              | 0.0      | 0.0%             |                      | 16.384K  | 0.00790297%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv6.0       | Conv2d        | (256,8,8)   | (256,8,8)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 10.928%          | #####                | 37.7651M | 18.2163%        | #########           |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv6.1       | BatchNorm2d   | (256,8,8)   | (256,8,8)   |                              | 512.0    | 0.00948203%      |                      | 32.768K  | 0.0158059%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv6.2       | ReLU          | (256,8,8)   | (256,8,8)   |                              | 0.0      | 0.0%             |                      | 16.384K  | 0.00790297%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv7.0       | Conv2d        | (256,8,8)   | (256,8,8)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 10.928%          | #####                | 37.7651M | 18.2163%        | #########           |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv7.1       | BatchNorm2d   | (256,8,8)   | (256,8,8)   |                              | 512.0    | 0.00948203%      |                      | 32.768K  | 0.0158059%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv7.2       | ReLU          | (256,8,8)   | (256,8,8)   |                              | 0.0      | 0.0%             |                      | 16.384K  | 0.00790297%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv7.3       | MaxPool2d     | (256,8,8)   | (256,4,4)   |                              | 0.0      | 0.0%             |                      | 16.384K  | 0.00790297%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv8.0       | Conv2d        | (256,4,4)   | (256,4,4)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 10.928%          | #####                | 9.44128M | 4.55409%        | ##                  |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv8.1       | BatchNorm2d   | (256,4,4)   | (256,4,4)   |                              | 512.0    | 0.00948203%      |                      | 8.192K   | 0.00395149%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv8.2       | ReLU          | (256,4,4)   | (256,4,4)   |                              | 0.0      | 0.0%             |                      | 4.096K   | 0.00197574%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv9.0       | Conv2d        | (256,4,4)   | (256,4,4)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 10.928%          | #####                | 9.44128M | 4.55409%        | ##                  |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv9.1       | BatchNorm2d   | (256,4,4)   | (256,4,4)   |                              | 512.0    | 0.00948203%      |                      | 8.192K   | 0.00395149%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv9.2       | ReLU          | (256,4,4)   | (256,4,4)   |                              | 0.0      | 0.0%             |                      | 4.096K   | 0.00197574%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv10.0      | Conv2d        | (256,4,4)   | (256,4,4)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 10.928%          | #####                | 9.44128M | 4.55409%        | ##                  |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv10.1      | BatchNorm2d   | (256,4,4)   | (256,4,4)   |                              | 512.0    | 0.00948203%      |                      | 8.192K   | 0.00395149%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv10.2      | ReLU          | (256,4,4)   | (256,4,4)   |                              | 0.0      | 0.0%             |                      | 4.096K   | 0.00197574%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv10.3      | MaxPool2d     | (256,4,4)   | (256,2,2)   |                              | 0.0      | 0.0%             |                      | 4.096K   | 0.00197574%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv11.0      | Conv2d        | (256,2,2)   | (256,2,2)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 10.928%          | #####                | 2.36032M | 1.13852%        |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv11.1      | BatchNorm2d   | (256,2,2)   | (256,2,2)   |                              | 512.0    | 0.00948203%      |                      | 2.048K   | 0.000987871%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv11.2      | ReLU          | (256,2,2)   | (256,2,2)   |                              | 0.0      | 0.0%             |                      | 1.024K   | 0.000493936%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv12.0      | Conv2d        | (256,2,2)   | (256,2,2)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 10.928%          | #####                | 2.36032M | 1.13852%        |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv12.1      | BatchNorm2d   | (256,2,2)   | (256,2,2)   |                              | 512.0    | 0.00948203%      |                      | 2.048K   | 0.000987871%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv12.2      | ReLU          | (256,2,2)   | (256,2,2)   |                              | 0.0      | 0.0%             |                      | 1.024K   | 0.000493936%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv13.0      | Conv2d        | (256,2,2)   | (256,2,2)   | k=(3, 3), p=(1, 1), s=(1, 1) | 590.08K  | 10.928%          | #####                | 2.36032M | 1.13852%        |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv13.1      | BatchNorm2d   | (256,2,2)   | (256,2,2)   |                              | 512.0    | 0.00948203%      |                      | 2.048K   | 0.000987871%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv13.2      | ReLU          | (256,2,2)   | (256,2,2)   |                              | 0.0      | 0.0%             |                      | 1.024K   | 0.000493936%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| conv13.3      | MaxPool2d     | (256,2,2)   | (256,1,1)   |                              | 0.0      | 0.0%             |                      | 1.024K   | 0.000493936%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc1.0         | Flatten       | (256,1,1)   | (256)       |                              | 0.0      | 0.0%             |                      | 0.0      | 0.0%            |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc1.1         | Linear        | (256)       | (512)       |                              | 131.584K | 2.43688%         | #                    | 131.072K | 0.0632238%      |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc1.2         | BatchNorm1d   | (512)       | (512)       |                              | 1.024K   | 0.0189641%       |                      | 1.024K   | 0.000493936%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc1.3         | ReLU          | (512)       | (512)       |                              | 0.0      | 0.0%             |                      | 512.0    | 0.000246968%    |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "| fc2.0         | Linear        | (512)       | (10)        |                              | 5.13K    | 0.0950055%       |                      | 5.12K    | 0.00246968%     |                     |\n",
      "+---------------+---------------+-------------+-------------+------------------------------+----------+------------------+----------------------+----------+-----------------+---------------------+\n",
      "\n",
      "\n",
      "FLOPs: 207.314M\n",
      "Params: 5.39969M\n"
     ]
    }
   ],
   "source": [
    "print(\"the number of flops after pruning\")\n",
    "! flopth -m VGG16_BN_PRUNE_FOR_SCRATCH -p ../architecture -i 3 32 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced percentage : 34.12%\n"
     ]
    }
   ],
   "source": [
    "num_flops_before_pruning = 314696000\n",
    "num_flops_after_pruning = 207314000\n",
    "\n",
    "print(f\"reduced percentage : {(num_flops_before_pruning - num_flops_after_pruning) / num_flops_before_pruning * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
