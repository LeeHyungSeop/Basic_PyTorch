2024-01-24 13:20:14.400618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-24 13:20:14.449484: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
mini_batch_size : 256
# threads : 8
# train examples : 1281167
# val examples : 50000
# train batches : 5005
# val batches : 196
check the structure of train_loader ----------------------
torch.Size([256, 3, 224, 224])
torch.Size([256])
0th class : tench
999th class : toilet_tissue
<bound method Module.eval of MyResNet50(
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): BottleneckBuildingBlock(
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv1x1_1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (1): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): BottleneckBuildingBlock(
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv1x1_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (1): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (3): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): BottleneckBuildingBlock(
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv1x1_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (1): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (3): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (4): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (5): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): BottleneckBuildingBlock(
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv1x1_1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (1): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer5): Sequential(
    (0): AdaptiveAvgPool2d(output_size=(1, 1))
    (1): Flatten(start_dim=1, end_dim=-1)
    (2): Linear(in_features=2048, out_features=1000, bias=True)
  )
)>
device : cuda:0
num_iters at 1 epoch: 5005
total num_iters: 600600
Adjusting learning rate of group 0 to 1.0000e-01.
# of total parameters : 25557032
# of trainable parameters : 25557032
Current Time : 2024-01-24 13:20:24.871968
1 / 120 epoch ----------------------------------------
[1, 1000th iteration] loss : 6.785870582103729
[1, 2000th iteration] loss : 5.94080900144577
[1, 3000th iteration] loss : 5.3247641248703
[1, 4000th iteration] loss : 4.8642235832214356
[1, 5000th iteration] loss : 4.474096469640732
val loss : 4.333368868243937
val acc : 15.738
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 14:04:45.925510
2 / 120 epoch ----------------------------------------
[2, 1000th iteration] loss : 4.132676839113236
[2, 2000th iteration] loss : 3.8786497662067414
[2, 3000th iteration] loss : 3.6499341342449187
[2, 4000th iteration] loss : 3.470062948465347
[2, 5000th iteration] loss : 3.3225294885635375
val loss : 3.5822400876453946
val acc : 26.766
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 14:49:05.408282
3 / 120 epoch ----------------------------------------
[3, 1000th iteration] loss : 3.154467715024948
[3, 2000th iteration] loss : 3.062050131559372
[3, 3000th iteration] loss : 2.9761996552944185
[3, 4000th iteration] loss : 2.9074842739105224
[3, 5000th iteration] loss : 2.8442812564373017
val loss : 2.8607056615303974
val acc : 37.114
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 15:33:24.138322
4 / 120 epoch ----------------------------------------
[4, 1000th iteration] loss : 2.73869512963295
[4, 2000th iteration] loss : 2.7207159566879273
[4, 3000th iteration] loss : 2.6781248009204863
[4, 4000th iteration] loss : 2.6382849321365356
[4, 5000th iteration] loss : 2.60276597738266
val loss : 2.8379983476230075
val acc : 37.82
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 16:17:41.855285
5 / 120 epoch ----------------------------------------
[5, 1000th iteration] loss : 2.5304001574516297
[5, 2000th iteration] loss : 2.5160586793422697
[5, 3000th iteration] loss : 2.49749183344841
[5, 4000th iteration] loss : 2.4735935492515564
[5, 5000th iteration] loss : 2.4532870154380797
val loss : 2.414132080516037
val acc : 44.776
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 17:01:56.955762
6 / 120 epoch ----------------------------------------
[6, 1000th iteration] loss : 2.3861275826692583
[6, 2000th iteration] loss : 2.400210725903511
[6, 3000th iteration] loss : 2.3850916957855226
[6, 4000th iteration] loss : 2.370589441776276
[6, 5000th iteration] loss : 2.340053865909576
val loss : 2.5119511156666037
val acc : 43.308
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 17:46:00.910679
7 / 120 epoch ----------------------------------------
[7, 1000th iteration] loss : 2.297360078573227
[7, 2000th iteration] loss : 2.302543730735779
[7, 3000th iteration] loss : 2.296330495715141
[7, 4000th iteration] loss : 2.2930336271524427
[7, 5000th iteration] loss : 2.272761606216431
val loss : 2.3055407879303913
val acc : 46.666
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 18:30:08.077297
8 / 120 epoch ----------------------------------------
[8, 1000th iteration] loss : 2.222119985342026
[8, 2000th iteration] loss : 2.230735655546188
[8, 3000th iteration] loss : 2.237569864869118
[8, 4000th iteration] loss : 2.2291303741931916
[8, 5000th iteration] loss : 2.232015944004059
val loss : 2.260380266272292
val acc : 47.554
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 19:14:18.341631
9 / 120 epoch ----------------------------------------
[9, 1000th iteration] loss : 2.175861535668373
[9, 2000th iteration] loss : 2.1847450798749923
[9, 3000th iteration] loss : 2.1799267209768294
[9, 4000th iteration] loss : 2.1862443224191668
[9, 5000th iteration] loss : 2.1883772283792498
val loss : 2.2052794311727797
val acc : 49.064
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 19:58:31.726235
10 / 120 epoch ----------------------------------------
[10, 1000th iteration] loss : 2.1263022123575213
[10, 2000th iteration] loss : 2.1558468774557116
[10, 3000th iteration] loss : 2.1477013206481934
[10, 4000th iteration] loss : 2.141559418439865
[10, 5000th iteration] loss : 2.1484472028017043
val loss : 2.183906187816542
val acc : 49.552
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 20:42:48.569060
11 / 120 epoch ----------------------------------------
[11, 1000th iteration] loss : 2.092787917137146
[11, 2000th iteration] loss : 2.1167094264030455
[11, 3000th iteration] loss : 2.1173114038705827
[11, 4000th iteration] loss : 2.133607960939407
[11, 5000th iteration] loss : 2.1229466247558593
val loss : 2.1608084367246043
val acc : 49.698
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 21:27:06.172654
12 / 120 epoch ----------------------------------------
[12, 1000th iteration] loss : 2.0757925745248795
[12, 2000th iteration] loss : 2.0895691056251526
[12, 3000th iteration] loss : 2.09597038769722
[12, 4000th iteration] loss : 2.09129974091053
[12, 5000th iteration] loss : 2.099316446185112
val loss : 2.2295728374500663
val acc : 48.652
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 22:11:22.398486
13 / 120 epoch ----------------------------------------
[13, 1000th iteration] loss : 2.04917726957798
[13, 2000th iteration] loss : 2.067829803228378
[13, 3000th iteration] loss : 2.0786877989768984
[13, 4000th iteration] loss : 2.0787314974069595
[13, 5000th iteration] loss : 2.0814401406049727
val loss : 2.1424538566141713
val acc : 50.282
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 22:55:28.422284
14 / 120 epoch ----------------------------------------
[14, 1000th iteration] loss : 2.030730252146721
[14, 2000th iteration] loss : 2.048138154745102
[14, 3000th iteration] loss : 2.061981930375099
[14, 4000th iteration] loss : 2.05970090675354
[14, 5000th iteration] loss : 2.059553918004036
val loss : 2.049917312300935
val acc : 51.708
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 23:39:34.494825
15 / 120 epoch ----------------------------------------
[15, 1000th iteration] loss : 2.020801608800888
[15, 2000th iteration] loss : 2.036657233595848
[15, 3000th iteration] loss : 2.0365245357751847
[15, 4000th iteration] loss : 2.043454491853714
[15, 5000th iteration] loss : 2.0456562416553496
val loss : 2.1502484478512587
val acc : 50.174
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 00:23:42.037433
16 / 120 epoch ----------------------------------------
[16, 1000th iteration] loss : 1.993380658507347
[16, 2000th iteration] loss : 2.0257603389024736
[16, 3000th iteration] loss : 2.027762168288231
[16, 4000th iteration] loss : 2.0289373600482943
[16, 5000th iteration] loss : 2.03697145652771
val loss : 2.2918949972610085
val acc : 47.448
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 01:07:48.816400
17 / 120 epoch ----------------------------------------
[17, 1000th iteration] loss : 1.9888986463546754
[17, 2000th iteration] loss : 2.0146564456224443
[17, 3000th iteration] loss : 2.0176094176769257
[17, 4000th iteration] loss : 2.0228754014968873
[17, 5000th iteration] loss : 2.019722614645958
val loss : 2.065353760305716
val acc : 51.956
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 01:51:55.446464
18 / 120 epoch ----------------------------------------
[18, 1000th iteration] loss : 1.976467553138733
[18, 2000th iteration] loss : 2.0066328083276748
[18, 3000th iteration] loss : 1.9998093074560166
[18, 4000th iteration] loss : 2.0179441418647768
[18, 5000th iteration] loss : 2.009175880074501
val loss : 2.1117383667400906
val acc : 51.05
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 02:36:01.738059
19 / 120 epoch ----------------------------------------
[19, 1000th iteration] loss : 1.9656778717041015
[19, 2000th iteration] loss : 1.9825078035593033
[19, 3000th iteration] loss : 2.001194851756096
[19, 4000th iteration] loss : 2.0018570083379745
[19, 5000th iteration] loss : 2.0081477459669115
val loss : 2.07217952426599
val acc : 51.516
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 03:20:07.305610
20 / 120 epoch ----------------------------------------
[20, 1000th iteration] loss : 1.9619809054136277
[20, 2000th iteration] loss : 1.9850055937767028
[20, 3000th iteration] loss : 1.9866757770776748
[20, 4000th iteration] loss : 1.9857781343460084
[20, 5000th iteration] loss : 1.998630282998085
val loss : 2.010113142582835
val acc : 52.568
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 04:04:12.116239
21 / 120 epoch ----------------------------------------
[21, 1000th iteration] loss : 1.9502925535440445
[21, 2000th iteration] loss : 1.975551944255829
[21, 3000th iteration] loss : 1.9752386810779572
[21, 4000th iteration] loss : 1.9831439498662948
[21, 5000th iteration] loss : 1.9975722326040268
val loss : 2.135294022000566
val acc : 50.366
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 04:48:17.433569
22 / 120 epoch ----------------------------------------
[22, 1000th iteration] loss : 1.9427095441818236
[22, 2000th iteration] loss : 1.9731351459026336
[22, 3000th iteration] loss : 1.9762228479385375
[22, 4000th iteration] loss : 1.9862302104234695
[22, 5000th iteration] loss : 1.9808532752990722
val loss : 2.0029308510069943
val acc : 53.336
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 05:32:22.609163
23 / 120 epoch ----------------------------------------
[23, 1000th iteration] loss : 1.936706934094429
[23, 2000th iteration] loss : 1.9714989985227585
[23, 3000th iteration] loss : 1.9608523092269898
[23, 4000th iteration] loss : 1.9734544731378556
[23, 5000th iteration] loss : 1.9816278628110886
val loss : 2.00491116302354
val acc : 53.196
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 06:16:27.649461
24 / 120 epoch ----------------------------------------
[24, 1000th iteration] loss : 1.9312948755025863
[24, 2000th iteration] loss : 1.958689185500145
[24, 3000th iteration] loss : 1.965699565410614
[24, 4000th iteration] loss : 1.9628913055658341
[24, 5000th iteration] loss : 1.9770142889022828
val loss : 2.0830636535372054
val acc : 51.608
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 07:00:32.327316
25 / 120 epoch ----------------------------------------
[25, 1000th iteration] loss : 1.9340957018136977
[25, 2000th iteration] loss : 1.9497255653142929
[25, 3000th iteration] loss : 1.9663678467273713
[25, 4000th iteration] loss : 1.9647390974760055
[25, 5000th iteration] loss : 1.9662010587453842
val loss : 2.048116177928691
val acc : 52.086
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 07:44:37.136556
26 / 120 epoch ----------------------------------------
[26, 1000th iteration] loss : 1.922617234826088
[26, 2000th iteration] loss : 1.949547152876854
[26, 3000th iteration] loss : 1.950901905655861
[26, 4000th iteration] loss : 1.964205708384514
[26, 5000th iteration] loss : 1.9717267154455185
val loss : 2.102581687727753
val acc : 51.036
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 08:28:41.024262
27 / 120 epoch ----------------------------------------
[27, 1000th iteration] loss : 1.9205157934427262
[27, 2000th iteration] loss : 1.930494749546051
[27, 3000th iteration] loss : 1.9581553971767425
[27, 4000th iteration] loss : 1.960023619055748
[27, 5000th iteration] loss : 1.962590567946434
val loss : 1.9853246315401427
val acc : 53.144
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 09:12:46.070391
28 / 120 epoch ----------------------------------------
[28, 1000th iteration] loss : 1.913580906867981
[28, 2000th iteration] loss : 1.9426004215478898
[28, 3000th iteration] loss : 1.9470659458637238
[28, 4000th iteration] loss : 1.9574436818361283
[28, 5000th iteration] loss : 1.9500533176660537
val loss : 2.07197213537839
val acc : 51.736
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 09:56:49.807123
29 / 120 epoch ----------------------------------------
[29, 1000th iteration] loss : 1.9153405559062957
[29, 2000th iteration] loss : 1.9352983944416047
[29, 3000th iteration] loss : 1.9445892795324327
[29, 4000th iteration] loss : 1.9502897112369537
[29, 5000th iteration] loss : 1.955833916425705
val loss : 2.155448228120804
val acc : 50.412
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 10:40:53.747883
30 / 120 epoch ----------------------------------------
[30, 1000th iteration] loss : 1.908854358434677
[30, 2000th iteration] loss : 1.9254469776153564
[30, 3000th iteration] loss : 1.9489759970903398
[30, 4000th iteration] loss : 1.9468952292203903
[30, 5000th iteration] loss : 1.955385494828224
val loss : 1.9874958912937009
val acc : 53.158
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 11:24:59.704349
31 / 120 epoch ----------------------------------------
[31, 1000th iteration] loss : 1.511687220454216
[31, 2000th iteration] loss : 1.39842139005661
[31, 3000th iteration] loss : 1.357912596821785
[31, 4000th iteration] loss : 1.3320808995962143
[31, 5000th iteration] loss : 1.3188195490837098
val loss : 1.2762167718337507
val acc : 68.282
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 12:09:04.798540
32 / 120 epoch ----------------------------------------
[32, 1000th iteration] loss : 1.2735379589796065
[32, 2000th iteration] loss : 1.269435105741024
[32, 3000th iteration] loss : 1.270231204032898
[32, 4000th iteration] loss : 1.260903453707695
[32, 5000th iteration] loss : 1.2505441893935203
val loss : 1.2352820087452323
val acc : 69.194
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 12:53:09.411018
33 / 120 epoch ----------------------------------------
[33, 1000th iteration] loss : 1.219800276041031
[33, 2000th iteration] loss : 1.2200244445204735
[33, 3000th iteration] loss : 1.218179117679596
[33, 4000th iteration] loss : 1.2207586464285851
[33, 5000th iteration] loss : 1.2159841554760933
val loss : 1.2111516068784558
val acc : 69.698
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 13:37:15.685575
34 / 120 epoch ----------------------------------------
[34, 1000th iteration] loss : 1.1843954395055771
[34, 2000th iteration] loss : 1.1820125810503959
[34, 3000th iteration] loss : 1.1868481165766716
[34, 4000th iteration] loss : 1.1913215392827987
[34, 5000th iteration] loss : 1.193592996954918
val loss : 1.1974820154053825
val acc : 70.09
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 14:21:22.172922
35 / 120 epoch ----------------------------------------
[35, 1000th iteration] loss : 1.1566738811135293
[35, 2000th iteration] loss : 1.1584262372255325
[35, 3000th iteration] loss : 1.1637003089785576
[35, 4000th iteration] loss : 1.1722420067191124
[35, 5000th iteration] loss : 1.177749734401703
val loss : 1.2019971502678735
val acc : 69.688
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 15:05:30.261235
36 / 120 epoch ----------------------------------------
[36, 1000th iteration] loss : 1.1330436407923699
[36, 2000th iteration] loss : 1.1490037863850593
[36, 3000th iteration] loss : 1.1546912276744843
[36, 4000th iteration] loss : 1.1577764735221863
[36, 5000th iteration] loss : 1.1601193919181825
val loss : 1.1941444569704485
val acc : 70.056
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 15:49:38.520915
37 / 120 epoch ----------------------------------------
[37, 1000th iteration] loss : 1.1180351253151894
[37, 2000th iteration] loss : 1.142058368265629
[37, 3000th iteration] loss : 1.1379079502224922
[37, 4000th iteration] loss : 1.1493924989104272
[37, 5000th iteration] loss : 1.1601383957266806
val loss : 1.1962299803081824
val acc : 69.954
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 16:33:47.490758
38 / 120 epoch ----------------------------------------
[38, 1000th iteration] loss : 1.1162804247140885
[38, 2000th iteration] loss : 1.127345273733139
[38, 3000th iteration] loss : 1.1308043447732925
[38, 4000th iteration] loss : 1.1443535329699517
[38, 5000th iteration] loss : 1.1543090807199479
val loss : 1.1989037066089863
val acc : 69.822
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 17:17:56.916144
39 / 120 epoch ----------------------------------------
[39, 1000th iteration] loss : 1.1054889366030693
[39, 2000th iteration] loss : 1.122404080748558
[39, 3000th iteration] loss : 1.1338159856796264
[39, 4000th iteration] loss : 1.136991490483284
[39, 5000th iteration] loss : 1.1457688353061677
val loss : 1.2265822391728967
val acc : 69.414
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 18:02:06.004049
40 / 120 epoch ----------------------------------------
[40, 1000th iteration] loss : 1.0969257445931435
[40, 2000th iteration] loss : 1.115712569773197
[40, 3000th iteration] loss : 1.1332841499447823
[40, 4000th iteration] loss : 1.133212995827198
[40, 5000th iteration] loss : 1.1483855407238006
val loss : 1.2015310951641627
val acc : 69.812
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 18:46:13.946341
41 / 120 epoch ----------------------------------------
[41, 1000th iteration] loss : 1.0996195086836815
[41, 2000th iteration] loss : 1.1125640268325805
[41, 3000th iteration] loss : 1.1297681556940078
[41, 4000th iteration] loss : 1.138342398762703
[41, 5000th iteration] loss : 1.1418082587718963
val loss : 1.205980617172864
val acc : 69.846
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 19:30:21.603467
42 / 120 epoch ----------------------------------------
[42, 1000th iteration] loss : 1.0974682187438012
[42, 2000th iteration] loss : 1.1109883125424385
[42, 3000th iteration] loss : 1.1291385278105737
[42, 4000th iteration] loss : 1.1308811967968941
[42, 5000th iteration] loss : 1.1430457234978675
val loss : 1.218518567024445
val acc : 69.864
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 20:14:30.846906
43 / 120 epoch ----------------------------------------
[43, 1000th iteration] loss : 1.094887696325779
[43, 2000th iteration] loss : 1.109769858956337
[43, 3000th iteration] loss : 1.124356118619442
[43, 4000th iteration] loss : 1.1424409979581833
[43, 5000th iteration] loss : 1.1391646774411202
val loss : 1.2011461759708366
val acc : 70.064
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 20:58:39.642691
44 / 120 epoch ----------------------------------------
[44, 1000th iteration] loss : 1.0949056116342544
[44, 2000th iteration] loss : 1.1100764566659926
[44, 3000th iteration] loss : 1.124399801492691
[44, 4000th iteration] loss : 1.1342691667675973
[44, 5000th iteration] loss : 1.1406507124900818
val loss : 1.2050246572007939
val acc : 70.024
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 21:42:50.049530
45 / 120 epoch ----------------------------------------
[45, 1000th iteration] loss : 1.092202982723713
[45, 2000th iteration] loss : 1.1100644501447678
[45, 3000th iteration] loss : 1.1185544249415398
[45, 4000th iteration] loss : 1.1305452591180802
[45, 5000th iteration] loss : 1.143050459563732
val loss : 1.2129702440329961
val acc : 69.894
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 22:27:01.952437
46 / 120 epoch ----------------------------------------
[46, 1000th iteration] loss : 1.089828814983368
[46, 2000th iteration] loss : 1.1111657165884972
[46, 3000th iteration] loss : 1.1221890622973443
[46, 4000th iteration] loss : 1.1300628670454025
[46, 5000th iteration] loss : 1.139679682135582
val loss : 1.2283149556237825
val acc : 69.392
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 23:11:13.066453
47 / 120 epoch ----------------------------------------
[47, 1000th iteration] loss : 1.086971668958664
[47, 2000th iteration] loss : 1.110207477927208
[47, 3000th iteration] loss : 1.1149232567548752
[47, 4000th iteration] loss : 1.1269994152784348
[47, 5000th iteration] loss : 1.1381531131267548
val loss : 1.2144197934136098
val acc : 69.714
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 23:55:22.768794
48 / 120 epoch ----------------------------------------
[48, 1000th iteration] loss : 1.0836239532232284
[48, 2000th iteration] loss : 1.112068663418293
[48, 3000th iteration] loss : 1.1130379576683045
[48, 4000th iteration] loss : 1.1216519886255265
[48, 5000th iteration] loss : 1.134166465342045
val loss : 1.2361101112803634
val acc : 69.172
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 00:39:33.838917
49 / 120 epoch ----------------------------------------
[49, 1000th iteration] loss : 1.0839818269610404
[49, 2000th iteration] loss : 1.0970458069443703
[49, 3000th iteration] loss : 1.122244345664978
[49, 4000th iteration] loss : 1.1228870390057564
[49, 5000th iteration] loss : 1.137882871389389
val loss : 1.2324476409323362
val acc : 69.534
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 01:23:43.802265
50 / 120 epoch ----------------------------------------
[50, 1000th iteration] loss : 1.08048391610384
[50, 2000th iteration] loss : 1.095070823609829
[50, 3000th iteration] loss : 1.1190633083581925
[50, 4000th iteration] loss : 1.1247908622026443
[50, 5000th iteration] loss : 1.1273787317872048
val loss : 1.2299213096195338
val acc : 69.384
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 02:07:53.650372
51 / 120 epoch ----------------------------------------
[51, 1000th iteration] loss : 1.0785514594316483
[51, 2000th iteration] loss : 1.0953461617231368
[51, 3000th iteration] loss : 1.1131010495424272
[51, 4000th iteration] loss : 1.119486910879612
[51, 5000th iteration] loss : 1.1223378086686133
val loss : 1.2231112067796746
val acc : 69.63
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 02:52:02.171387
52 / 120 epoch ----------------------------------------
[52, 1000th iteration] loss : 1.069539798617363
[52, 2000th iteration] loss : 1.0961623921394348
[52, 3000th iteration] loss : 1.1057519887685776
[52, 4000th iteration] loss : 1.1182371203303336
[52, 5000th iteration] loss : 1.1266724197268485
val loss : 1.2394953135933195
val acc : 69.226
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 03:36:09.147389
53 / 120 epoch ----------------------------------------
[53, 1000th iteration] loss : 1.0669559742212296
[53, 2000th iteration] loss : 1.0893022698760033
[53, 3000th iteration] loss : 1.1097062531113624
[53, 4000th iteration] loss : 1.1117909427285195
[53, 5000th iteration] loss : 1.1220014620423318
val loss : 1.2486771342097496
val acc : 69.022
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 04:20:16.943849
54 / 120 epoch ----------------------------------------
[54, 1000th iteration] loss : 1.0702093955278396
[54, 2000th iteration] loss : 1.0870597027540208
[54, 3000th iteration] loss : 1.098985093653202
[54, 4000th iteration] loss : 1.1116659502983093
[54, 5000th iteration] loss : 1.1196220986247063
val loss : 1.2380797431177022
val acc : 69.264
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 05:04:23.946616
55 / 120 epoch ----------------------------------------
[55, 1000th iteration] loss : 1.0619502106308938
[55, 2000th iteration] loss : 1.0878936744332313
[55, 3000th iteration] loss : 1.1001983523964882
[55, 4000th iteration] loss : 1.1063864827156067
[55, 5000th iteration] loss : 1.1153804733753205
val loss : 1.2473890799648908
val acc : 68.984
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 05:48:31.685321
56 / 120 epoch ----------------------------------------
[56, 1000th iteration] loss : 1.0558605619668961
[56, 2000th iteration] loss : 1.08169054377079
[56, 3000th iteration] loss : 1.0953401844501496
[56, 4000th iteration] loss : 1.1107249921560287
[56, 5000th iteration] loss : 1.1091176272630692
val loss : 1.2589541263118083
val acc : 68.808
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 06:32:38.943799
57 / 120 epoch ----------------------------------------
[57, 1000th iteration] loss : 1.0546605330705643
[57, 2000th iteration] loss : 1.0695961260795592
[57, 3000th iteration] loss : 1.0931355165243148
[57, 4000th iteration] loss : 1.0993933277130128
[57, 5000th iteration] loss : 1.111665304481983
val loss : 1.2081008894102914
val acc : 69.936
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 07:16:44.334228
58 / 120 epoch ----------------------------------------
[58, 1000th iteration] loss : 1.0522815048098564
[58, 2000th iteration] loss : 1.078628315448761
[58, 3000th iteration] loss : 1.0912750951051713
[58, 4000th iteration] loss : 1.1000476413369178
[58, 5000th iteration] loss : 1.100107851088047

RESUME TRAINING EPOCH 55 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

2024-01-26 15:47:42.310500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-26 15:47:42.358075: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
device : cuda:0
num_iters at 1 epoch: 5005
total num_iters: 600600
Adjusting learning rate of group 0 to 1.0000e-02.
epoch : 54
Current Time : 2024-01-26 15:47:46.869701
55 / 120 epoch ----------------------------------------
