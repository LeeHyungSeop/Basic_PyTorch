2024-01-24 13:20:14.400618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-24 13:20:14.449484: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
mini_batch_size : 256
# threads : 8
# train examples : 1281167
# val examples : 50000
# train batches : 5005
# val batches : 196
check the structure of train_loader ----------------------
torch.Size([256, 3, 224, 224])
torch.Size([256])
0th class : tench
999th class : toilet_tissue
<bound method Module.eval of MyResNet50(
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): BottleneckBuildingBlock(
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv1x1_1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (1): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): BottleneckBuildingBlock(
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv1x1_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (1): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (3): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): BottleneckBuildingBlock(
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv1x1_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (1): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (3): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (4): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (5): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): BottleneckBuildingBlock(
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv1x1_1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (1): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): BottleneckBuildingBlock(
      (conv1x1_1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv3x3_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv1x1_3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer5): Sequential(
    (0): AdaptiveAvgPool2d(output_size=(1, 1))
    (1): Flatten(start_dim=1, end_dim=-1)
    (2): Linear(in_features=2048, out_features=1000, bias=True)
  )
)>
device : cuda:0
num_iters at 1 epoch: 5005
total num_iters: 600600
Adjusting learning rate of group 0 to 1.0000e-01.
# of total parameters : 25557032
# of trainable parameters : 25557032
Current Time : 2024-01-24 13:20:24.871968
1 / 120 epoch ----------------------------------------
[1, 1000th iteration] loss : 6.785870582103729
[1, 2000th iteration] loss : 5.94080900144577
[1, 3000th iteration] loss : 5.3247641248703
[1, 4000th iteration] loss : 4.8642235832214356
[1, 5000th iteration] loss : 4.474096469640732
val loss : 4.333368868243937
val acc : 15.738
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 14:04:45.925510
2 / 120 epoch ----------------------------------------
[2, 1000th iteration] loss : 4.132676839113236
[2, 2000th iteration] loss : 3.8786497662067414
[2, 3000th iteration] loss : 3.6499341342449187
[2, 4000th iteration] loss : 3.470062948465347
[2, 5000th iteration] loss : 3.3225294885635375
val loss : 3.5822400876453946
val acc : 26.766
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 14:49:05.408282
3 / 120 epoch ----------------------------------------
[3, 1000th iteration] loss : 3.154467715024948
[3, 2000th iteration] loss : 3.062050131559372
[3, 3000th iteration] loss : 2.9761996552944185
[3, 4000th iteration] loss : 2.9074842739105224
[3, 5000th iteration] loss : 2.8442812564373017
val loss : 2.8607056615303974
val acc : 37.114
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 15:33:24.138322
4 / 120 epoch ----------------------------------------
[4, 1000th iteration] loss : 2.73869512963295
[4, 2000th iteration] loss : 2.7207159566879273
[4, 3000th iteration] loss : 2.6781248009204863
[4, 4000th iteration] loss : 2.6382849321365356
[4, 5000th iteration] loss : 2.60276597738266
val loss : 2.8379983476230075
val acc : 37.82
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 16:17:41.855285
5 / 120 epoch ----------------------------------------
[5, 1000th iteration] loss : 2.5304001574516297
[5, 2000th iteration] loss : 2.5160586793422697
[5, 3000th iteration] loss : 2.49749183344841
[5, 4000th iteration] loss : 2.4735935492515564
[5, 5000th iteration] loss : 2.4532870154380797
val loss : 2.414132080516037
val acc : 44.776
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 17:01:56.955762
6 / 120 epoch ----------------------------------------
[6, 1000th iteration] loss : 2.3861275826692583
[6, 2000th iteration] loss : 2.400210725903511
[6, 3000th iteration] loss : 2.3850916957855226
[6, 4000th iteration] loss : 2.370589441776276
[6, 5000th iteration] loss : 2.340053865909576
val loss : 2.5119511156666037
val acc : 43.308
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 17:46:00.910679
7 / 120 epoch ----------------------------------------
[7, 1000th iteration] loss : 2.297360078573227
[7, 2000th iteration] loss : 2.302543730735779
[7, 3000th iteration] loss : 2.296330495715141
[7, 4000th iteration] loss : 2.2930336271524427
[7, 5000th iteration] loss : 2.272761606216431
val loss : 2.3055407879303913
val acc : 46.666
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 18:30:08.077297
8 / 120 epoch ----------------------------------------
[8, 1000th iteration] loss : 2.222119985342026
[8, 2000th iteration] loss : 2.230735655546188
[8, 3000th iteration] loss : 2.237569864869118
[8, 4000th iteration] loss : 2.2291303741931916
[8, 5000th iteration] loss : 2.232015944004059
val loss : 2.260380266272292
val acc : 47.554
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 19:14:18.341631
9 / 120 epoch ----------------------------------------
[9, 1000th iteration] loss : 2.175861535668373
[9, 2000th iteration] loss : 2.1847450798749923
[9, 3000th iteration] loss : 2.1799267209768294
[9, 4000th iteration] loss : 2.1862443224191668
[9, 5000th iteration] loss : 2.1883772283792498
val loss : 2.2052794311727797
val acc : 49.064
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 19:58:31.726235
10 / 120 epoch ----------------------------------------
[10, 1000th iteration] loss : 2.1263022123575213
[10, 2000th iteration] loss : 2.1558468774557116
[10, 3000th iteration] loss : 2.1477013206481934
[10, 4000th iteration] loss : 2.141559418439865
[10, 5000th iteration] loss : 2.1484472028017043
val loss : 2.183906187816542
val acc : 49.552
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 20:42:48.569060
11 / 120 epoch ----------------------------------------
[11, 1000th iteration] loss : 2.092787917137146
[11, 2000th iteration] loss : 2.1167094264030455
[11, 3000th iteration] loss : 2.1173114038705827
[11, 4000th iteration] loss : 2.133607960939407
[11, 5000th iteration] loss : 2.1229466247558593
val loss : 2.1608084367246043
val acc : 49.698
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 21:27:06.172654
12 / 120 epoch ----------------------------------------
[12, 1000th iteration] loss : 2.0757925745248795
[12, 2000th iteration] loss : 2.0895691056251526
[12, 3000th iteration] loss : 2.09597038769722
[12, 4000th iteration] loss : 2.09129974091053
[12, 5000th iteration] loss : 2.099316446185112
val loss : 2.2295728374500663
val acc : 48.652
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 22:11:22.398486
13 / 120 epoch ----------------------------------------
[13, 1000th iteration] loss : 2.04917726957798
[13, 2000th iteration] loss : 2.067829803228378
[13, 3000th iteration] loss : 2.0786877989768984
[13, 4000th iteration] loss : 2.0787314974069595
[13, 5000th iteration] loss : 2.0814401406049727
val loss : 2.1424538566141713
val acc : 50.282
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 22:55:28.422284
14 / 120 epoch ----------------------------------------
[14, 1000th iteration] loss : 2.030730252146721
[14, 2000th iteration] loss : 2.048138154745102
[14, 3000th iteration] loss : 2.061981930375099
[14, 4000th iteration] loss : 2.05970090675354
[14, 5000th iteration] loss : 2.059553918004036
val loss : 2.049917312300935
val acc : 51.708
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-24 23:39:34.494825
15 / 120 epoch ----------------------------------------
[15, 1000th iteration] loss : 2.020801608800888
[15, 2000th iteration] loss : 2.036657233595848
[15, 3000th iteration] loss : 2.0365245357751847
[15, 4000th iteration] loss : 2.043454491853714
[15, 5000th iteration] loss : 2.0456562416553496
val loss : 2.1502484478512587
val acc : 50.174
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 00:23:42.037433
16 / 120 epoch ----------------------------------------
[16, 1000th iteration] loss : 1.993380658507347
[16, 2000th iteration] loss : 2.0257603389024736
[16, 3000th iteration] loss : 2.027762168288231
[16, 4000th iteration] loss : 2.0289373600482943
[16, 5000th iteration] loss : 2.03697145652771
val loss : 2.2918949972610085
val acc : 47.448
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 01:07:48.816400
17 / 120 epoch ----------------------------------------
[17, 1000th iteration] loss : 1.9888986463546754
[17, 2000th iteration] loss : 2.0146564456224443
[17, 3000th iteration] loss : 2.0176094176769257
[17, 4000th iteration] loss : 2.0228754014968873
[17, 5000th iteration] loss : 2.019722614645958
val loss : 2.065353760305716
val acc : 51.956
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 01:51:55.446464
18 / 120 epoch ----------------------------------------
[18, 1000th iteration] loss : 1.976467553138733
[18, 2000th iteration] loss : 2.0066328083276748
[18, 3000th iteration] loss : 1.9998093074560166
[18, 4000th iteration] loss : 2.0179441418647768
[18, 5000th iteration] loss : 2.009175880074501
val loss : 2.1117383667400906
val acc : 51.05
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 02:36:01.738059
19 / 120 epoch ----------------------------------------
[19, 1000th iteration] loss : 1.9656778717041015
[19, 2000th iteration] loss : 1.9825078035593033
[19, 3000th iteration] loss : 2.001194851756096
[19, 4000th iteration] loss : 2.0018570083379745
[19, 5000th iteration] loss : 2.0081477459669115
val loss : 2.07217952426599
val acc : 51.516
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 03:20:07.305610
20 / 120 epoch ----------------------------------------
[20, 1000th iteration] loss : 1.9619809054136277
[20, 2000th iteration] loss : 1.9850055937767028
[20, 3000th iteration] loss : 1.9866757770776748
[20, 4000th iteration] loss : 1.9857781343460084
[20, 5000th iteration] loss : 1.998630282998085
val loss : 2.010113142582835
val acc : 52.568
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 04:04:12.116239
21 / 120 epoch ----------------------------------------
[21, 1000th iteration] loss : 1.9502925535440445
[21, 2000th iteration] loss : 1.975551944255829
[21, 3000th iteration] loss : 1.9752386810779572
[21, 4000th iteration] loss : 1.9831439498662948
[21, 5000th iteration] loss : 1.9975722326040268
val loss : 2.135294022000566
val acc : 50.366
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 04:48:17.433569
22 / 120 epoch ----------------------------------------
[22, 1000th iteration] loss : 1.9427095441818236
[22, 2000th iteration] loss : 1.9731351459026336
[22, 3000th iteration] loss : 1.9762228479385375
[22, 4000th iteration] loss : 1.9862302104234695
[22, 5000th iteration] loss : 1.9808532752990722
val loss : 2.0029308510069943
val acc : 53.336
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 05:32:22.609163
23 / 120 epoch ----------------------------------------
[23, 1000th iteration] loss : 1.936706934094429
[23, 2000th iteration] loss : 1.9714989985227585
[23, 3000th iteration] loss : 1.9608523092269898
[23, 4000th iteration] loss : 1.9734544731378556
[23, 5000th iteration] loss : 1.9816278628110886
val loss : 2.00491116302354
val acc : 53.196
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 06:16:27.649461
24 / 120 epoch ----------------------------------------
[24, 1000th iteration] loss : 1.9312948755025863
[24, 2000th iteration] loss : 1.958689185500145
[24, 3000th iteration] loss : 1.965699565410614
[24, 4000th iteration] loss : 1.9628913055658341
[24, 5000th iteration] loss : 1.9770142889022828
val loss : 2.0830636535372054
val acc : 51.608
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 07:00:32.327316
25 / 120 epoch ----------------------------------------
[25, 1000th iteration] loss : 1.9340957018136977
[25, 2000th iteration] loss : 1.9497255653142929
[25, 3000th iteration] loss : 1.9663678467273713
[25, 4000th iteration] loss : 1.9647390974760055
[25, 5000th iteration] loss : 1.9662010587453842
val loss : 2.048116177928691
val acc : 52.086
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 07:44:37.136556
26 / 120 epoch ----------------------------------------
[26, 1000th iteration] loss : 1.922617234826088
[26, 2000th iteration] loss : 1.949547152876854
[26, 3000th iteration] loss : 1.950901905655861
[26, 4000th iteration] loss : 1.964205708384514
[26, 5000th iteration] loss : 1.9717267154455185
val loss : 2.102581687727753
val acc : 51.036
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 08:28:41.024262
27 / 120 epoch ----------------------------------------
[27, 1000th iteration] loss : 1.9205157934427262
[27, 2000th iteration] loss : 1.930494749546051
[27, 3000th iteration] loss : 1.9581553971767425
[27, 4000th iteration] loss : 1.960023619055748
[27, 5000th iteration] loss : 1.962590567946434
val loss : 1.9853246315401427
val acc : 53.144
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 09:12:46.070391
28 / 120 epoch ----------------------------------------
[28, 1000th iteration] loss : 1.913580906867981
[28, 2000th iteration] loss : 1.9426004215478898
[28, 3000th iteration] loss : 1.9470659458637238
[28, 4000th iteration] loss : 1.9574436818361283
[28, 5000th iteration] loss : 1.9500533176660537
val loss : 2.07197213537839
val acc : 51.736
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 09:56:49.807123
29 / 120 epoch ----------------------------------------
[29, 1000th iteration] loss : 1.9153405559062957
[29, 2000th iteration] loss : 1.9352983944416047
[29, 3000th iteration] loss : 1.9445892795324327
[29, 4000th iteration] loss : 1.9502897112369537
[29, 5000th iteration] loss : 1.955833916425705
val loss : 2.155448228120804
val acc : 50.412
Adjusting learning rate of group 0 to 1.0000e-01.
Current Time : 2024-01-25 10:40:53.747883
30 / 120 epoch ----------------------------------------
[30, 1000th iteration] loss : 1.908854358434677
[30, 2000th iteration] loss : 1.9254469776153564
[30, 3000th iteration] loss : 1.9489759970903398
[30, 4000th iteration] loss : 1.9468952292203903
[30, 5000th iteration] loss : 1.955385494828224
val loss : 1.9874958912937009
val acc : 53.158
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 11:24:59.704349
31 / 120 epoch ----------------------------------------
[31, 1000th iteration] loss : 1.511687220454216
[31, 2000th iteration] loss : 1.39842139005661
[31, 3000th iteration] loss : 1.357912596821785
[31, 4000th iteration] loss : 1.3320808995962143
[31, 5000th iteration] loss : 1.3188195490837098
val loss : 1.2762167718337507
val acc : 68.282
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 12:09:04.798540
32 / 120 epoch ----------------------------------------
[32, 1000th iteration] loss : 1.2735379589796065
[32, 2000th iteration] loss : 1.269435105741024
[32, 3000th iteration] loss : 1.270231204032898
[32, 4000th iteration] loss : 1.260903453707695
[32, 5000th iteration] loss : 1.2505441893935203
val loss : 1.2352820087452323
val acc : 69.194
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 12:53:09.411018
33 / 120 epoch ----------------------------------------
[33, 1000th iteration] loss : 1.219800276041031
[33, 2000th iteration] loss : 1.2200244445204735
[33, 3000th iteration] loss : 1.218179117679596
[33, 4000th iteration] loss : 1.2207586464285851
[33, 5000th iteration] loss : 1.2159841554760933
val loss : 1.2111516068784558
val acc : 69.698
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 13:37:15.685575
34 / 120 epoch ----------------------------------------
[34, 1000th iteration] loss : 1.1843954395055771
[34, 2000th iteration] loss : 1.1820125810503959
[34, 3000th iteration] loss : 1.1868481165766716
[34, 4000th iteration] loss : 1.1913215392827987
[34, 5000th iteration] loss : 1.193592996954918
val loss : 1.1974820154053825
val acc : 70.09
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 14:21:22.172922
35 / 120 epoch ----------------------------------------
[35, 1000th iteration] loss : 1.1566738811135293
[35, 2000th iteration] loss : 1.1584262372255325
[35, 3000th iteration] loss : 1.1637003089785576
[35, 4000th iteration] loss : 1.1722420067191124
[35, 5000th iteration] loss : 1.177749734401703
val loss : 1.2019971502678735
val acc : 69.688
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 15:05:30.261235
36 / 120 epoch ----------------------------------------
[36, 1000th iteration] loss : 1.1330436407923699
[36, 2000th iteration] loss : 1.1490037863850593
[36, 3000th iteration] loss : 1.1546912276744843
[36, 4000th iteration] loss : 1.1577764735221863
[36, 5000th iteration] loss : 1.1601193919181825
val loss : 1.1941444569704485
val acc : 70.056
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 15:49:38.520915
37 / 120 epoch ----------------------------------------
[37, 1000th iteration] loss : 1.1180351253151894
[37, 2000th iteration] loss : 1.142058368265629
[37, 3000th iteration] loss : 1.1379079502224922
[37, 4000th iteration] loss : 1.1493924989104272
[37, 5000th iteration] loss : 1.1601383957266806
val loss : 1.1962299803081824
val acc : 69.954
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 16:33:47.490758
38 / 120 epoch ----------------------------------------
[38, 1000th iteration] loss : 1.1162804247140885
[38, 2000th iteration] loss : 1.127345273733139
[38, 3000th iteration] loss : 1.1308043447732925
[38, 4000th iteration] loss : 1.1443535329699517
[38, 5000th iteration] loss : 1.1543090807199479
val loss : 1.1989037066089863
val acc : 69.822
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 17:17:56.916144
39 / 120 epoch ----------------------------------------
[39, 1000th iteration] loss : 1.1054889366030693
[39, 2000th iteration] loss : 1.122404080748558
[39, 3000th iteration] loss : 1.1338159856796264
[39, 4000th iteration] loss : 1.136991490483284
[39, 5000th iteration] loss : 1.1457688353061677
val loss : 1.2265822391728967
val acc : 69.414
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 18:02:06.004049
40 / 120 epoch ----------------------------------------
[40, 1000th iteration] loss : 1.0969257445931435
[40, 2000th iteration] loss : 1.115712569773197
[40, 3000th iteration] loss : 1.1332841499447823
[40, 4000th iteration] loss : 1.133212995827198
[40, 5000th iteration] loss : 1.1483855407238006
val loss : 1.2015310951641627
val acc : 69.812
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 18:46:13.946341
41 / 120 epoch ----------------------------------------
[41, 1000th iteration] loss : 1.0996195086836815
[41, 2000th iteration] loss : 1.1125640268325805
[41, 3000th iteration] loss : 1.1297681556940078
[41, 4000th iteration] loss : 1.138342398762703
[41, 5000th iteration] loss : 1.1418082587718963
val loss : 1.205980617172864
val acc : 69.846
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 19:30:21.603467
42 / 120 epoch ----------------------------------------
[42, 1000th iteration] loss : 1.0974682187438012
[42, 2000th iteration] loss : 1.1109883125424385
[42, 3000th iteration] loss : 1.1291385278105737
[42, 4000th iteration] loss : 1.1308811967968941
[42, 5000th iteration] loss : 1.1430457234978675
val loss : 1.218518567024445
val acc : 69.864
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 20:14:30.846906
43 / 120 epoch ----------------------------------------
[43, 1000th iteration] loss : 1.094887696325779
[43, 2000th iteration] loss : 1.109769858956337
[43, 3000th iteration] loss : 1.124356118619442
[43, 4000th iteration] loss : 1.1424409979581833
[43, 5000th iteration] loss : 1.1391646774411202
val loss : 1.2011461759708366
val acc : 70.064
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 20:58:39.642691
44 / 120 epoch ----------------------------------------
[44, 1000th iteration] loss : 1.0949056116342544
[44, 2000th iteration] loss : 1.1100764566659926
[44, 3000th iteration] loss : 1.124399801492691
[44, 4000th iteration] loss : 1.1342691667675973
[44, 5000th iteration] loss : 1.1406507124900818
val loss : 1.2050246572007939
val acc : 70.024
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 21:42:50.049530
45 / 120 epoch ----------------------------------------
[45, 1000th iteration] loss : 1.092202982723713
[45, 2000th iteration] loss : 1.1100644501447678
[45, 3000th iteration] loss : 1.1185544249415398
[45, 4000th iteration] loss : 1.1305452591180802
[45, 5000th iteration] loss : 1.143050459563732
val loss : 1.2129702440329961
val acc : 69.894
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 22:27:01.952437
46 / 120 epoch ----------------------------------------
[46, 1000th iteration] loss : 1.089828814983368
[46, 2000th iteration] loss : 1.1111657165884972
[46, 3000th iteration] loss : 1.1221890622973443
[46, 4000th iteration] loss : 1.1300628670454025
[46, 5000th iteration] loss : 1.139679682135582
val loss : 1.2283149556237825
val acc : 69.392
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 23:11:13.066453
47 / 120 epoch ----------------------------------------
[47, 1000th iteration] loss : 1.086971668958664
[47, 2000th iteration] loss : 1.110207477927208
[47, 3000th iteration] loss : 1.1149232567548752
[47, 4000th iteration] loss : 1.1269994152784348
[47, 5000th iteration] loss : 1.1381531131267548
val loss : 1.2144197934136098
val acc : 69.714
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-25 23:55:22.768794
48 / 120 epoch ----------------------------------------
[48, 1000th iteration] loss : 1.0836239532232284
[48, 2000th iteration] loss : 1.112068663418293
[48, 3000th iteration] loss : 1.1130379576683045
[48, 4000th iteration] loss : 1.1216519886255265
[48, 5000th iteration] loss : 1.134166465342045
val loss : 1.2361101112803634
val acc : 69.172
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 00:39:33.838917
49 / 120 epoch ----------------------------------------
[49, 1000th iteration] loss : 1.0839818269610404
[49, 2000th iteration] loss : 1.0970458069443703
[49, 3000th iteration] loss : 1.122244345664978
[49, 4000th iteration] loss : 1.1228870390057564
[49, 5000th iteration] loss : 1.137882871389389
val loss : 1.2324476409323362
val acc : 69.534
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 01:23:43.802265
50 / 120 epoch ----------------------------------------
[50, 1000th iteration] loss : 1.08048391610384
[50, 2000th iteration] loss : 1.095070823609829
[50, 3000th iteration] loss : 1.1190633083581925
[50, 4000th iteration] loss : 1.1247908622026443
[50, 5000th iteration] loss : 1.1273787317872048
val loss : 1.2299213096195338
val acc : 69.384
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 02:07:53.650372
51 / 120 epoch ----------------------------------------
[51, 1000th iteration] loss : 1.0785514594316483
[51, 2000th iteration] loss : 1.0953461617231368
[51, 3000th iteration] loss : 1.1131010495424272
[51, 4000th iteration] loss : 1.119486910879612
[51, 5000th iteration] loss : 1.1223378086686133
val loss : 1.2231112067796746
val acc : 69.63
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 02:52:02.171387
52 / 120 epoch ----------------------------------------
[52, 1000th iteration] loss : 1.069539798617363
[52, 2000th iteration] loss : 1.0961623921394348
[52, 3000th iteration] loss : 1.1057519887685776
[52, 4000th iteration] loss : 1.1182371203303336
[52, 5000th iteration] loss : 1.1266724197268485
val loss : 1.2394953135933195
val acc : 69.226
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 03:36:09.147389
53 / 120 epoch ----------------------------------------
[53, 1000th iteration] loss : 1.0669559742212296
[53, 2000th iteration] loss : 1.0893022698760033
[53, 3000th iteration] loss : 1.1097062531113624
[53, 4000th iteration] loss : 1.1117909427285195
[53, 5000th iteration] loss : 1.1220014620423318
val loss : 1.2486771342097496
val acc : 69.022
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 04:20:16.943849
54 / 120 epoch ----------------------------------------
[54, 1000th iteration] loss : 1.0702093955278396
[54, 2000th iteration] loss : 1.0870597027540208
[54, 3000th iteration] loss : 1.098985093653202
[54, 4000th iteration] loss : 1.1116659502983093
[54, 5000th iteration] loss : 1.1196220986247063
val loss : 1.2380797431177022
val acc : 69.264
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 05:04:23.946616
55 / 120 epoch ----------------------------------------
[55, 1000th iteration] loss : 1.0619502106308938
[55, 2000th iteration] loss : 1.0878936744332313
[55, 3000th iteration] loss : 1.1001983523964882
[55, 4000th iteration] loss : 1.1063864827156067
[55, 5000th iteration] loss : 1.1153804733753205
val loss : 1.2473890799648908
val acc : 68.984
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 05:48:31.685321
56 / 120 epoch ----------------------------------------
[56, 1000th iteration] loss : 1.0558605619668961
[56, 2000th iteration] loss : 1.08169054377079
[56, 3000th iteration] loss : 1.0953401844501496
[56, 4000th iteration] loss : 1.1107249921560287
[56, 5000th iteration] loss : 1.1091176272630692
val loss : 1.2589541263118083
val acc : 68.808
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 06:32:38.943799
57 / 120 epoch ----------------------------------------
[57, 1000th iteration] loss : 1.0546605330705643
[57, 2000th iteration] loss : 1.0695961260795592
[57, 3000th iteration] loss : 1.0931355165243148
[57, 4000th iteration] loss : 1.0993933277130128
[57, 5000th iteration] loss : 1.111665304481983
val loss : 1.2081008894102914
val acc : 69.936
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 07:16:44.334228
58 / 120 epoch ----------------------------------------
[58, 1000th iteration] loss : 1.0522815048098564
[58, 2000th iteration] loss : 1.078628315448761
[58, 3000th iteration] loss : 1.0912750951051713
[58, 4000th iteration] loss : 1.1000476413369178
[58, 5000th iteration] loss : 1.100107851088047

RESUME TRAINING EPOCH 55 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
2024-01-26 17:50:24.256604: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-26 17:50:24.365362: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
device : cuda:0
num_iters at 1 epoch: 5005
total num_iters: 600600
Adjusting learning rate of group 0 to 1.0000e-02.
epoch : 54
Current Time : 2024-01-26 17:50:29.981202
55 / 120 epoch ----------------------------------------
[55, 1000th iteration] loss : 1.0556839415431023
[55, 2000th iteration] loss : 1.0775058321356774
[55, 3000th iteration] loss : 1.093665739953518
[55, 4000th iteration] loss : 1.1011198258399963
[55, 5000th iteration] loss : 1.1086414668560027
val loss : 1.214105295587559
val acc : 69.912
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 18:33:43.296795
56 / 120 epoch ----------------------------------------
[56, 1000th iteration] loss : 1.0522021979093552
[56, 2000th iteration] loss : 1.0712513780593873
[56, 3000th iteration] loss : 1.0887854176163674
[56, 4000th iteration] loss : 1.102628974378109
[56, 5000th iteration] loss : 1.1041038007736206
val loss : 1.2162868046030706
val acc : 69.65
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 19:16:55.074535
57 / 120 epoch ----------------------------------------
[57, 1000th iteration] loss : 1.0468230382204056
[57, 2000th iteration] loss : 1.0701795043349267
[57, 3000th iteration] loss : 1.0886092545986175
[57, 4000th iteration] loss : 1.0933577917814254
[57, 5000th iteration] loss : 1.104591654241085
val loss : 1.242332107254437
val acc : 69.21
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 20:00:04.744673
58 / 120 epoch ----------------------------------------
[58, 1000th iteration] loss : 1.0458169068098069
[58, 2000th iteration] loss : 1.0689539616107941
[58, 3000th iteration] loss : 1.0798884162306785
[58, 4000th iteration] loss : 1.0926006754040718
[58, 5000th iteration] loss : 1.1015810072422028
val loss : 1.2541103487720295
val acc : 68.912
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 20:43:13.141012
59 / 120 epoch ----------------------------------------
[59, 1000th iteration] loss : 1.043553438782692
[59, 2000th iteration] loss : 1.0673711366057397
[59, 3000th iteration] loss : 1.0758984592556953
[59, 4000th iteration] loss : 1.0880487011671067
[59, 5000th iteration] loss : 1.096752595603466
val loss : 1.232661736863
val acc : 69.412
Adjusting learning rate of group 0 to 1.0000e-02.
Current Time : 2024-01-26 21:26:22.005597
60 / 120 epoch ----------------------------------------
[60, 1000th iteration] loss : 1.0403129445314407
[60, 2000th iteration] loss : 1.0627880103588103
[60, 3000th iteration] loss : 1.074211250782013
[60, 4000th iteration] loss : 1.089654941856861
[60, 5000th iteration] loss : 1.0909289691448212
val loss : 1.2468325510925176
val acc : 69.294
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-26 22:09:31.154297
61 / 120 epoch ----------------------------------------
[61, 1000th iteration] loss : 0.9112368919849396
[61, 2000th iteration] loss : 0.8614129377603531
[61, 3000th iteration] loss : 0.8467300324440002
[61, 4000th iteration] loss : 0.8313105694651604
[61, 5000th iteration] loss : 0.825178528368473
val loss : 1.0368043470139405
val acc : 73.886
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-26 22:52:39.444361
62 / 120 epoch ----------------------------------------
[62, 1000th iteration] loss : 0.7993701666593551
[62, 2000th iteration] loss : 0.7988691780567169
[62, 3000th iteration] loss : 0.7986737378239631
[62, 4000th iteration] loss : 0.7912452374696731
[62, 5000th iteration] loss : 0.7897233287990093
val loss : 1.0237199396503216
val acc : 74.244
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-26 23:35:48.120067
63 / 120 epoch ----------------------------------------
[63, 1000th iteration] loss : 0.7746406270265579
[63, 2000th iteration] loss : 0.7730326476097107
[63, 3000th iteration] loss : 0.7727501934170723
[63, 4000th iteration] loss : 0.7757056807279586
[63, 5000th iteration] loss : 0.7732975063323975
val loss : 1.0205938505883119
val acc : 74.3
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 00:18:56.324227
64 / 120 epoch ----------------------------------------
[64, 1000th iteration] loss : 0.7574602820873261
[64, 2000th iteration] loss : 0.7552336636781692
[64, 3000th iteration] loss : 0.7579214398264885
[64, 4000th iteration] loss : 0.7590488086342811
[64, 5000th iteration] loss : 0.7564383940696716
val loss : 1.0159097253059854
val acc : 74.51
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 01:02:12.663344
65 / 120 epoch ----------------------------------------
[65, 1000th iteration] loss : 0.7445055929124356
[65, 2000th iteration] loss : 0.747683744430542
[65, 3000th iteration] loss : 0.7428239569664001
[65, 4000th iteration] loss : 0.7438337501883506
[65, 5000th iteration] loss : 0.7442721217274666
val loss : 1.0122983580341145
val acc : 74.55
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 01:45:53.341700
66 / 120 epoch ----------------------------------------
[66, 1000th iteration] loss : 0.7335837613940239
[66, 2000th iteration] loss : 0.7322868306040764
[66, 3000th iteration] loss : 0.7339492912590504
[66, 4000th iteration] loss : 0.7388655334711075
[66, 5000th iteration] loss : 0.7362274435162545
val loss : 1.0104468084719715
val acc : 74.554
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 02:29:03.623981
67 / 120 epoch ----------------------------------------
[67, 1000th iteration] loss : 0.7156676141619682
[67, 2000th iteration] loss : 0.7203700650036335
[67, 3000th iteration] loss : 0.7262069283425808
[67, 4000th iteration] loss : 0.7263764933943748
[67, 5000th iteration] loss : 0.7278612234592438
val loss : 1.0135349759033747
val acc : 74.53
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 03:12:11.935232
68 / 120 epoch ----------------------------------------
[68, 1000th iteration] loss : 0.7083287724256515
[68, 2000th iteration] loss : 0.7097064417898655
[68, 3000th iteration] loss : 0.7159939348697663
[68, 4000th iteration] loss : 0.7127212339341641
[68, 5000th iteration] loss : 0.7228353090882301
val loss : 1.0106158630580317
val acc : 74.646
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 03:55:19.717900
69 / 120 epoch ----------------------------------------
[69, 1000th iteration] loss : 0.7006763094961643
[69, 2000th iteration] loss : 0.7090082544386387
[69, 3000th iteration] loss : 0.7050044050812722
[69, 4000th iteration] loss : 0.7102606190741062
[69, 5000th iteration] loss : 0.7084394714534282
val loss : 1.010401256534518
val acc : 74.772
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 04:38:29.089615
70 / 120 epoch ----------------------------------------
[70, 1000th iteration] loss : 0.6944324902594089
[70, 2000th iteration] loss : 0.6912332441210747
[70, 3000th iteration] loss : 0.7015032350718975
[70, 4000th iteration] loss : 0.6963120687603951
[70, 5000th iteration] loss : 0.7035272315442562
val loss : 1.0108241815956271
val acc : 74.776
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 05:21:38.213010
71 / 120 epoch ----------------------------------------
[71, 1000th iteration] loss : 0.6840228272080422
[71, 2000th iteration] loss : 0.6862301893234253
[71, 3000th iteration] loss : 0.6925445161461831
[71, 4000th iteration] loss : 0.6961903974413872
[71, 5000th iteration] loss : 0.6930461319684982
val loss : 1.0112726207898588
val acc : 74.672
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 06:04:46.475526
72 / 120 epoch ----------------------------------------
[72, 1000th iteration] loss : 0.6791088879704476
[72, 2000th iteration] loss : 0.6772282745838165
[72, 3000th iteration] loss : 0.6874067948758602
[72, 4000th iteration] loss : 0.6905684778094292
[72, 5000th iteration] loss : 0.6903641470372677
val loss : 1.0089130213066024
val acc : 74.916
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 06:47:55.170376
73 / 120 epoch ----------------------------------------
[73, 1000th iteration] loss : 0.6707180696725845
[73, 2000th iteration] loss : 0.6742914099097251
[73, 3000th iteration] loss : 0.6797237738370895
[73, 4000th iteration] loss : 0.6855724532604217
[73, 5000th iteration] loss : 0.6800894026458263
val loss : 1.009501546925428
val acc : 74.89
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 07:31:03.303133
74 / 120 epoch ----------------------------------------
[74, 1000th iteration] loss : 0.6655008725523949
[74, 2000th iteration] loss : 0.6716041340231895
[74, 3000th iteration] loss : 0.6743940548598766
[74, 4000th iteration] loss : 0.6733677670359611
[74, 5000th iteration] loss : 0.6783275734782219
val loss : 1.0176676885814082
val acc : 74.688
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 08:14:11.860242
75 / 120 epoch ----------------------------------------
[75, 1000th iteration] loss : 0.6593991277813911
[75, 2000th iteration] loss : 0.6606857880651951
[75, 3000th iteration] loss : 0.6673404907882213
[75, 4000th iteration] loss : 0.6723868943750858
[75, 5000th iteration] loss : 0.6732959205508232
val loss : 1.01404612374549
val acc : 74.752
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 08:57:19.722695
76 / 120 epoch ----------------------------------------
[76, 1000th iteration] loss : 0.6529195406734943
[76, 2000th iteration] loss : 0.6547741037905216
[76, 3000th iteration] loss : 0.6641110927760601
[76, 4000th iteration] loss : 0.662758750140667
[76, 5000th iteration] loss : 0.6648338772058487
val loss : 1.0167071965275978
val acc : 74.726
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 09:40:28.330140
77 / 120 epoch ----------------------------------------
[77, 1000th iteration] loss : 0.6480686358809471
[77, 2000th iteration] loss : 0.6521185324490071
[77, 3000th iteration] loss : 0.6539291631579399
[77, 4000th iteration] loss : 0.6609385541081428
[77, 5000th iteration] loss : 0.6652031890749931
val loss : 1.0177907566634976
val acc : 74.554
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 10:23:36.604239
78 / 120 epoch ----------------------------------------
[78, 1000th iteration] loss : 0.6420265930891037
[78, 2000th iteration] loss : 0.6486268554031849
[78, 3000th iteration] loss : 0.6512284598052501
[78, 4000th iteration] loss : 0.6582561597824097
[78, 5000th iteration] loss : 0.6547681284248829
val loss : 1.0167681285921408
val acc : 74.602
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 11:06:42.599813
79 / 120 epoch ----------------------------------------
[79, 1000th iteration] loss : 0.6378862952589989
[79, 2000th iteration] loss : 0.642056282132864
[79, 3000th iteration] loss : 0.6452677884399891
[79, 4000th iteration] loss : 0.6480301311314106
[79, 5000th iteration] loss : 0.6536217814087868
val loss : 1.0207929705478707
val acc : 74.73
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 11:49:48.504938
80 / 120 epoch ----------------------------------------
[80, 1000th iteration] loss : 0.6326215322315693
[80, 2000th iteration] loss : 0.6400410610437394
[80, 3000th iteration] loss : 0.645293811827898
[80, 4000th iteration] loss : 0.6464795945882797
[80, 5000th iteration] loss : 0.6464890019595623
val loss : 1.0244596141333482
val acc : 74.466
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 12:32:56.011809
81 / 120 epoch ----------------------------------------
[81, 1000th iteration] loss : 0.6255090597569942
[81, 2000th iteration] loss : 0.633055985301733
[81, 3000th iteration] loss : 0.6345338698327542
[81, 4000th iteration] loss : 0.6406596365571022
[81, 5000th iteration] loss : 0.6455053643882275
val loss : 1.0232191405126028
val acc : 74.658
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 13:16:04.335796
82 / 120 epoch ----------------------------------------
[82, 1000th iteration] loss : 0.6240302357077598
[82, 2000th iteration] loss : 0.6259748762249947
[82, 3000th iteration] loss : 0.6313356886804103
[82, 4000th iteration] loss : 0.6347007417082786
[82, 5000th iteration] loss : 0.6411772286593914
val loss : 1.0304452889427846
val acc : 74.342
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 13:59:13.221742
83 / 120 epoch ----------------------------------------
[83, 1000th iteration] loss : 0.619972293138504
[83, 2000th iteration] loss : 0.6223957007527351
[83, 3000th iteration] loss : 0.6280616324841977
[83, 4000th iteration] loss : 0.6299648597538471
[83, 5000th iteration] loss : 0.6365086366832257
val loss : 1.0252546570738967
val acc : 74.63
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 14:42:22.903796
84 / 120 epoch ----------------------------------------
[84, 1000th iteration] loss : 0.6135515332520008
[84, 2000th iteration] loss : 0.6187594444155693
[84, 3000th iteration] loss : 0.62468448138237
[84, 4000th iteration] loss : 0.6291362780630588
[84, 5000th iteration] loss : 0.6306921785771846
val loss : 1.0306126785521605
val acc : 74.45
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 15:25:34.951132
85 / 120 epoch ----------------------------------------
[85, 1000th iteration] loss : 0.6100909055769443
[85, 2000th iteration] loss : 0.616568924844265
[85, 3000th iteration] loss : 0.6169263785779476
[85, 4000th iteration] loss : 0.625703190535307
[85, 5000th iteration] loss : 0.6249012971818447
val loss : 1.0340142925174869
val acc : 74.458
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 16:08:49.434418
86 / 120 epoch ----------------------------------------
[86, 1000th iteration] loss : 0.6056025202870369
[86, 2000th iteration] loss : 0.6069913012981415
[86, 3000th iteration] loss : 0.6143016466498374
[86, 4000th iteration] loss : 0.6192635615468025
[86, 5000th iteration] loss : 0.6248989452719689
val loss : 1.0285001929317201
val acc : 74.5
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 16:52:03.002940
87 / 120 epoch ----------------------------------------
[87, 1000th iteration] loss : 0.6056653785407543
[87, 2000th iteration] loss : 0.6068098721802234
[87, 3000th iteration] loss : 0.6105922468900681
[87, 4000th iteration] loss : 0.615512424916029
[87, 5000th iteration] loss : 0.6194114907979965
val loss : 1.0366846286520666
val acc : 74.268
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 17:35:18.037411
88 / 120 epoch ----------------------------------------
[88, 1000th iteration] loss : 0.5955004174411297
[88, 2000th iteration] loss : 0.6067408698797226
[88, 3000th iteration] loss : 0.6077066289484501
[88, 4000th iteration] loss : 0.6093485835492611
[88, 5000th iteration] loss : 0.6106913995742798
val loss : 1.0332035939304196
val acc : 74.458
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 18:18:47.219033
89 / 120 epoch ----------------------------------------
[89, 1000th iteration] loss : 0.5927237023413181
[89, 2000th iteration] loss : 0.6007309489548206
[89, 3000th iteration] loss : 0.6038951080739499
[89, 4000th iteration] loss : 0.6118274441957474
[89, 5000th iteration] loss : 0.6067585898935794
val loss : 1.0437548507233054
val acc : 74.38
Adjusting learning rate of group 0 to 1.0000e-03.
Current Time : 2024-01-27 19:01:59.965042
90 / 120 epoch ----------------------------------------
[90, 1000th iteration] loss : 0.5881058068871499
[90, 2000th iteration] loss : 0.5953049723505974
[90, 3000th iteration] loss : 0.5999511971473693
[90, 4000th iteration] loss : 0.601536505907774
[90, 5000th iteration] loss : 0.6090483160614968
val loss : 1.0467926537504002
val acc : 74.232
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-27 19:45:09.769132
91 / 120 epoch ----------------------------------------
[91, 1000th iteration] loss : 0.5758124247789383
[91, 2000th iteration] loss : 0.566914828568697
[91, 3000th iteration] loss : 0.5619340486228466
[91, 4000th iteration] loss : 0.563795346468687
[91, 5000th iteration] loss : 0.5598802942335606
val loss : 1.0218740850687027
val acc : 74.774
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-27 20:28:18.779471
92 / 120 epoch ----------------------------------------
[92, 1000th iteration] loss : 0.5568694717884064
[92, 2000th iteration] loss : 0.5531424527168274
[92, 3000th iteration] loss : 0.5545894856154918
[92, 4000th iteration] loss : 0.5540482813715935
[92, 5000th iteration] loss : 0.5557738836407662
val loss : 1.0236454651672013
val acc : 74.748
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-27 21:11:27.740139
93 / 120 epoch ----------------------------------------
[93, 1000th iteration] loss : 0.5476866400837899
[93, 2000th iteration] loss : 0.5535296810567379
[93, 3000th iteration] loss : 0.5491730461716652
[93, 4000th iteration] loss : 0.5476618984341621
[93, 5000th iteration] loss : 0.551041234344244
val loss : 1.0209461918898992
val acc : 74.856
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-27 21:54:36.788203
94 / 120 epoch ----------------------------------------
[94, 1000th iteration] loss : 0.5463621585667133
[94, 2000th iteration] loss : 0.5467276573181152
[94, 3000th iteration] loss : 0.547335242062807
[94, 4000th iteration] loss : 0.5512976678907872
[94, 5000th iteration] loss : 0.5484174566864968
val loss : 1.017687236472052
val acc : 74.938
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-27 22:37:46.250431
95 / 120 epoch ----------------------------------------
[95, 1000th iteration] loss : 0.5429525532722473
[95, 2000th iteration] loss : 0.5444082967340946
[95, 3000th iteration] loss : 0.5462445783913136
[95, 4000th iteration] loss : 0.5470819094777107
[95, 5000th iteration] loss : 0.5424007964730263
val loss : 1.0222381620990986
val acc : 74.87
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-27 23:20:55.227000

RESUME TRAINING EPOCH 95 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
2024-01-28 22:08:41.092335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-28 22:08:41.226525: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
device : cuda:0
num_iters at 1 epoch: 5005
total num_iters: 600600
Adjusting learning rate of group 0 to 1.0000e-04.
epoch : 114
Current Time : 2024-01-28 22:08:46.711022
116 / 120 epoch ----------------------------------------
[116, 1000th iteration] loss : 0.5202297658920289
[116, 2000th iteration] loss : 0.5216236179471015
[116, 3000th iteration] loss : 0.5206618504524231
[116, 4000th iteration] loss : 0.5236370983421803
[116, 5000th iteration] loss : 0.5245137945413589
val loss : 1.0300259945952162
val acc : 74.81
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-28 22:51:59.171628
117 / 120 epoch ----------------------------------------
[117, 1000th iteration] loss : 0.5190660753548145
[117, 2000th iteration] loss : 0.5220008684694767
[117, 3000th iteration] loss : 0.5198240622580051
[117, 4000th iteration] loss : 0.5221322965919971
[117, 5000th iteration] loss : 0.5232904989719391
val loss : 1.026307826139489
val acc : 74.806
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-28 23:35:07.973233
118 / 120 epoch ----------------------------------------
[118, 1000th iteration] loss : 0.516981139689684
[118, 2000th iteration] loss : 0.520659398585558
[118, 3000th iteration] loss : 0.5260858730375767
[118, 4000th iteration] loss : 0.5254669880867004
[118, 5000th iteration] loss : 0.5199676546454429
val loss : 1.0273419320583344
val acc : 74.812
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-29 00:18:15.997284
119 / 120 epoch ----------------------------------------
[119, 1000th iteration] loss : 0.5209137416183949
[119, 2000th iteration] loss : 0.5208983144760132
[119, 3000th iteration] loss : 0.5207282919287681
[119, 4000th iteration] loss : 0.5195895202159881
[119, 5000th iteration] loss : 0.524348390430212
val loss : 1.0264259926518615
val acc : 74.894
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-29 01:01:23.847413
120 / 120 epoch ----------------------------------------
[120, 1000th iteration] loss : 0.5175884657204152
[120, 2000th iteration] loss : 0.5171010346114635
[120, 3000th iteration] loss : 0.5167038733065128
[120, 4000th iteration] loss : 0.5169549345076084
[120, 5000th iteration] loss : 0.5232581359446049
val loss : 1.0272573752670873
val acc : 74.782
Adjusting learning rate of group 0 to 1.0000e-04.
Current Time : 2024-01-29 01:44:32.282858
~~~ Training Finished ~~~
[In Paper] top-1 acc : 77.15 %, top-5 acc : 93.29 %
num_val_batch : 196
[inference exp1] : transforms.Resize(256)
[Top-1]
val loss : 3.6900517225873712
val acc : 76.45%
error rate : 23.55%
[Top-5]
val loss : 3.6842682001237965
val acc : 93.248%
error rate : 6.752%
[inference exp2] : transforms.Resize((256 + 480) // 2)
[Top-1]
val loss : 4.05346449601407
val acc : 75.52%
error rate : 24.48%
[Top-5]
val loss : 4.052355338724292
val acc : 92.762%
error rate : 7.238%
