{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.5979e+37,  4.5754e-41, -6.5979e+37])\n",
      "tensor([[1.3764e+34, 3.0683e-41, 1.3810e+34],\n",
      "        [3.0683e-41, 4.2599e-43, 0.0000e+00]])\n",
      "tensor([[[[-6.5980e+37,  4.5754e-41, -6.5980e+37],\n",
      "          [ 4.5754e-41,  1.4013e-45,  0.0000e+00]],\n",
      "\n",
      "         [[ 1.4013e-45,  0.0000e+00,  1.4013e-45],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 8.9683e-44,  0.0000e+00,  1.5695e-43],\n",
      "          [ 0.0000e+00,  1.3790e+34,  3.0683e-41]],\n",
      "\n",
      "         [[ 2.6540e-09,  1.6706e-07,  1.0633e-05],\n",
      "          [ 1.0930e-05,  2.1707e-18,  1.6678e+19]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.empty(3) # Rank 1 Tensor, 3\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(2, 3) # Rank 2 Tensor, 2 x 3\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(2, 2, 2, 3) # Rank 4 Tensor, 2 x 2 x 2 x 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "torch.float32\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float16)\n",
      "torch.float16\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 2)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2, 2, dtype=torch.float16)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.5, 0.1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operatoions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## element-wise addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0416, 0.7575],\n",
      "        [0.7350, 0.1557]])\n",
      "tensor([[0.4163, 0.0382],\n",
      "        [0.5382, 0.0180]])\n",
      "tensor([[0.4579, 0.7957],\n",
      "        [1.2732, 0.1736]])\n",
      "tensor([[0.4579, 0.7957],\n",
      "        [1.2732, 0.1736]])\n",
      "tensor([[0.4579, 0.7957],\n",
      "        [1.2732, 0.1736]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = x + y # element-wise addition\n",
    "print(z)\n",
    "\n",
    "z = torch.add(x, y) # element-wise addition\n",
    "print(z)\n",
    "\n",
    "y.add_(x) # in-place addition. PyTorch에서 모든 underbar는 in-place를 의미.\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## element-wise substraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4340, 0.9046],\n",
      "        [0.2238, 0.4649]])\n",
      "tensor([[0.0986, 0.7191],\n",
      "        [0.0016, 0.7388]])\n",
      "tensor([[ 0.3354,  0.1855],\n",
      "        [ 0.2222, -0.2740]])\n",
      "tensor([[ 0.3354,  0.1855],\n",
      "        [ 0.2222, -0.2740]])\n",
      "tensor([[ 0.3354,  0.1855],\n",
      "        [ 0.2222, -0.2740]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = x - y # element-wise substraction\n",
    "print(z)\n",
    "\n",
    "z = torch.sub(x, y) # element-wise substraction\n",
    "print(z)\n",
    "\n",
    "x.sub_(y) # in-place substraction. PyTorch에서 모든 underbar는 in-place를 의미.\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6951, 0.9090],\n",
      "        [0.2309, 0.8292]])\n",
      "tensor([[0.2746, 0.7299],\n",
      "        [0.3157, 0.8014]])\n",
      "tensor([[0.1909, 0.6635],\n",
      "        [0.0729, 0.6646]])\n",
      "tensor([[0.1909, 0.6635],\n",
      "        [0.0729, 0.6646]])\n",
      "tensor([[0.1909, 0.6635],\n",
      "        [0.0729, 0.6646]])\n",
      "tensor([[0.1909, 0.6635],\n",
      "        [0.0729, 0.6646]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = x * y # element-wise multiplication\n",
    "print(z)\n",
    "\n",
    "z = torch.mul(x, y) # element-wise multiplication\n",
    "print(z)\n",
    "\n",
    "z = torch.multiply(x, y) # element-wise multiplication\n",
    "print(z)\n",
    "\n",
    "x.mul_(y) # in-place multiplication. PyTorch에서 모든 underbar는 in-place를 의미.\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## element-wise division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9828, 0.2628],\n",
      "        [0.8056, 0.8566]])\n",
      "tensor([[0.9799, 0.2575],\n",
      "        [0.9965, 0.5600]])\n",
      "tensor([[1.0029, 1.0209],\n",
      "        [0.8085, 1.5296]])\n",
      "tensor([[1.0029, 1.0209],\n",
      "        [0.8085, 1.5296]])\n",
      "tensor([[1.0029, 1.0209],\n",
      "        [0.8085, 1.5296]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = x / y # element-wise multiplication\n",
    "print(z)\n",
    "\n",
    "z = torch.div(x, y) # element-wise multiplication\n",
    "print(z)\n",
    "\n",
    "x.div_(y) # in-place multiplication. PyTorch에서 모든 underbar는 in-place를 의미.\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9947, 0.3079, 0.3668],\n",
      "        [0.9226, 0.6017, 0.2594],\n",
      "        [0.1888, 0.5213, 0.1577],\n",
      "        [0.9080, 0.4297, 0.7105],\n",
      "        [0.7422, 0.7178, 0.7640]])\n",
      "tensor([0.9947, 0.9226, 0.1888, 0.9080, 0.7422])\n",
      "tensor([0.9226, 0.6017, 0.2594])\n",
      "tensor(0.6017)\n",
      "0.6016807556152344\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "print(x[:, 0]) # all rows, column 0\n",
    "print(x[1, :]) # row 1, all columns\n",
    "print(x[1, 1]) # row 1, column 1\n",
    "print(x[1, 1].item()) # actual value (element가 딱 하나 있는 tensor에서만 사용 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Operations(reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7841, 0.2714, 0.9357, 0.6280],\n",
      "        [0.9764, 0.0857, 0.8134, 0.2502],\n",
      "        [0.5157, 0.0941, 0.7687, 0.0265],\n",
      "        [0.1741, 0.7468, 0.8573, 0.4340]])\n",
      "tensor([0.7841, 0.2714, 0.9357, 0.6280, 0.9764, 0.0857, 0.8134, 0.2502, 0.5157,\n",
      "        0.0941, 0.7687, 0.0265, 0.1741, 0.7468, 0.8573, 0.4340])\n",
      "torch.Size([16])\n",
      "tensor([[0.7841, 0.2714, 0.9357, 0.6280, 0.9764, 0.0857, 0.8134, 0.2502, 0.5157,\n",
      "         0.0941, 0.7687, 0.0265, 0.1741, 0.7468, 0.8573, 0.4340]])\n",
      "torch.Size([1, 16])\n",
      "tensor([[[0.7841, 0.2714, 0.9357, 0.6280],\n",
      "         [0.9764, 0.0857, 0.8134, 0.2502]],\n",
      "\n",
      "        [[0.5157, 0.0941, 0.7687, 0.0265],\n",
      "         [0.1741, 0.7468, 0.8573, 0.4340]]])\n",
      "torch.Size([2, 2, 4])\n",
      "tensor([0.7841, 0.2714, 0.9357, 0.6280, 0.9764, 0.0857, 0.8134, 0.2502, 0.5157,\n",
      "        0.0941, 0.7687, 0.0265, 0.1741, 0.7468, 0.8573, 0.4340])\n",
      "torch.Size([16])\n",
      "tensor([[0.7841, 0.2714, 0.9357, 0.6280, 0.9764, 0.0857, 0.8134, 0.2502],\n",
      "        [0.5157, 0.0941, 0.7687, 0.0265, 0.1741, 0.7468, 0.8573, 0.4340]])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4, 4)\n",
    "print(x)\n",
    "y = x.view(16) # reshape\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "y = x.view(1, 16) # reshape\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "y = x.view(2, 2, 4) # reshape\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "y = x.view(-1)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "y = x.view(-1, 8) # -1 : automatically infer the number of rows\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting NumPy to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'torch.Tensor'>\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "print(type(a)) # torch.Tensor\n",
    "\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b)) # numpy.ndarray\n",
    "\n",
    "\n",
    "# ! Tensor가 CPU에 있다면, a와 b는 같은 memory allocation을 공유 !\n",
    "a.add_(1)\n",
    "print(a) # \n",
    "print(b) # b에도 add_(1) 연산이 적용되었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "\n",
    "a += 1 # numpy array에 element-wise 1 addition\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA and How to create tensor on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # CUDA device object\n",
    "    x = torch.ones(5, device=device) # directly create a tensor on GPU\n",
    "    print(x)\n",
    "    \n",
    "    y = torch.ones(5) # create a tensor on CPU\n",
    "    y = y.to(device) # move tensor to GPU\n",
    "    print(y)\n",
    "    \n",
    "    z = x + y # add two tensors on GPU\n",
    "    # z.numpy() # error!! numpy tensor는 CPU에서만 동작함.\n",
    "    \n",
    "    z = z.to(\"cpu\") # move tensor to CPU\n",
    "    z.numpy() # z는 이제 CPU에 있으므로 numpy() 사용 가능\n",
    "    print(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
