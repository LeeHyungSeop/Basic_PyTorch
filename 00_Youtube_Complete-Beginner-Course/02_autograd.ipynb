{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Calculation\n",
    "\n",
    "* Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5981, -0.8342,  1.3817], requires_grad=True)\n",
      "tensor([0.4019, 1.1658, 3.3817], grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3230,  2.7184, 22.8725], grad_fn=<MulBackward0>)\n",
      "tensor(8.6379, grad_fn=<MeanBackward0>)\n",
      "tensor([0.5358, 1.5545, 4.5090])\n"
     ]
    }
   ],
   "source": [
    "# optimization을 위해 gradient를 계산하고 싶은 tensor에는 requires_grad=True를 설정해줘야 함. (default = False)\n",
    "# pytorch에서 x에 대한 gradient를 tracking하기 위해 x.grad에 gradient를 저장함.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x) \n",
    "\n",
    "y = x + 2 # \n",
    "print(y) # grad_fn=<AddBackward0>\n",
    "\n",
    "z = y * y * 2\n",
    "print(z) # grad_fn=<MulBackward0>\n",
    "\n",
    "# vector Jacobian product(= chain rule) to get the gradient of the loss w.r.t. the input x\n",
    "# so in this case since our z is a scalar, the output of this operation will be a vector of the same shape as x\n",
    "z = z.mean() # (1, 3)을 (1, 1) = scalar로 만들어주기 위함. (1, 3)이었다면 Error.\n",
    "print(z) # grad_fn=<MeanBackward0>\n",
    "\n",
    "z.backward() # dz/dx\n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1240, -0.8386,  0.2818], requires_grad=True)\n",
      "tensor([1.8760, 1.1614, 2.2818], grad_fn=<AddBackward0>)\n",
      "tensor([ 7.0389,  2.6977, 10.4135], grad_fn=<MulBackward0>)\n",
      "None\n",
      "tensor([0.7504, 4.6456, 0.0091])\n"
     ]
    }
   ],
   "source": [
    "# optimization을 위해 gradient를 계산하고 싶은 tensor에는 requires_grad=True를 설정해줘야 함. (default = False)\n",
    "# pytorch에서 x에 대한 gradient를 tracking하기 위해 x.grad에 gradient를 저장함.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x) \n",
    "\n",
    "y = x + 2 \n",
    "print(y) # grad_fn=<AddBackward0>\n",
    "\n",
    "z = y * y * 2\n",
    "print(z) # grad_fn=<MulBackward0>\n",
    "\n",
    "# z = z.mean() # (1, 3) \n",
    "\n",
    "# z와 같은 shape의 tensor를 만들어줌.\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32) \n",
    "\n",
    "z.backward(v) \n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Gradient History\n",
    "\n",
    "* Three ways how we can stop pytorch from creating the gradient functions and tracking the history in our computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3992, -0.5867,  0.0301], requires_grad=True)\n",
      "tensor([-1.3992, -0.5867,  0.0301])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "x.requires_grad_(False) # in-place operation\n",
    "print(x) # requires_grad = True가 이제 보이지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6700, -1.7655,  0.6098], requires_grad=True)\n",
      "tensor([ 0.6700, -1.7655,  0.6098], requires_grad=True)\n",
      "tensor([ 0.6700, -1.7655,  0.6098])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "x.detach() # requires_grad = False인 tensor를 return함.\n",
    "print(x) \n",
    "\n",
    "y  = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. with torch.no_grad() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4035,  0.4721,  1.4661], requires_grad=True)\n",
      "tensor([1.5965, 2.4721, 3.4661], grad_fn=<AddBackward0>)\n",
      "tensor([1.5965, 2.4721, 3.4661])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad() : # with문 안에서는 gradient를 계산하지 않음.\n",
    "    y = x + 2\n",
    "    print(y) # grad_fn=<AddBackward0>이 여기서는 나오지 않음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .backward() 주의사항 : (acuumulated grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# .backward() will accumulate gradients into .grad instead of overwriting\n",
    "# pytorch에서는 gradient를 더해주는 것이 default임.\n",
    "# 그래서 정확한 gradient를 얻지 못하게 된다.\n",
    "for epoch in range(3) :\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# 따라서 정확한 gradient를 얻기 위해서는 .backward()를 호출할 때마다 .grad를 0으로 초기화해줘야 함.\n",
    "for epoch in range(3) :\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad) \n",
    "    \n",
    "    weights.grad.zero_() # gradient를 0으로 초기화해줘야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
