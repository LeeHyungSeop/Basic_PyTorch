{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Calculation\n",
    "\n",
    "* Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9087,  0.7818,  0.1101], requires_grad=True)\n",
      "tensor([1.0913, 2.7818, 2.1101], grad_fn=<AddBackward0>)\n",
      "tensor([ 2.3821, 15.4772,  8.9046], grad_fn=<MulBackward0>)\n",
      "tensor(8.9213, grad_fn=<MeanBackward0>)\n",
      "z.grad : None\n",
      "y.grad : None\n",
      "x.grad : tensor([1.4551, 3.7091, 2.8134])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hslee/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352660876/work/build/aten/src/ATen/core/TensorBody.h:477.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# optimization을 위해 gradient를 계산하고 싶은 tensor에는 requires_grad=True를 설정해줘야 함. (default = False)\n",
    "# pytorch에서 x에 대한 gradient를 tracking하기 위해 x.grad에 gradient를 저장함.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x) \n",
    "\n",
    "y = x + 2 # \n",
    "print(y) # grad_fn=<AddBackward0>\n",
    "\n",
    "z = y * y * 2\n",
    "print(z) # grad_fn=<MulBackward0>\n",
    "\n",
    "# vector Jacobian product(= chain rule) to get the gradient of the loss w.r.t. the input x\n",
    "# so in this case since our z is a scalar, the output of this operation will be a vector of the same shape as x\n",
    "z = z.mean() # (1, 3)을 (1, 1) = scalar로 만들어주기 위함. (1, 3)이었다면 Error.\n",
    "print(z) # grad_fn=<MeanBackward0>\n",
    "\n",
    "z.backward() # dz/dx\n",
    "\n",
    "print(f\"z.grad : {z.grad}\")\n",
    "print(f\"y.grad : {y.grad}\") \n",
    "print(f\"x.grad : {x.grad}\") # dz/dx = 2y = 2(x+2) = 2(1, 1, 1) = (2, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6201, -1.5607,  1.2418], requires_grad=True)\n",
      "tensor([2.6201, 0.4393, 3.2418], grad_fn=<AddBackward0>)\n",
      "tensor([13.7302,  0.3859, 21.0183], grad_fn=<MulBackward0>)\n",
      "tensor([1.0481, 1.7570, 0.0130])\n"
     ]
    }
   ],
   "source": [
    "# optimization을 위해 gradient를 계산하고 싶은 tensor에는 requires_grad=True를 설정해줘야 함. (default = False)\n",
    "# pytorch에서 x에 대한 gradient를 tracking하기 위해 x.grad에 gradient를 저장함.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x) \n",
    "\n",
    "y = x + 2 \n",
    "print(y) # grad_fn=<AddBackward0>\n",
    "\n",
    "z = y * y * 2\n",
    "print(z) # grad_fn=<MulBackward0>\n",
    "\n",
    "# z = z.mean() # (1, 3) \n",
    "\n",
    "# z와 같은 shape의 tensor를 만들어줌.\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32) \n",
    "\n",
    "z.backward(v) \n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Gradient History\n",
    "\n",
    "* Three ways how we can stop pytorch from creating the gradient functions and tracking the history in our computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0232, -0.3053, -0.6070], requires_grad=True)\n",
      "tensor([ 0.0232, -0.3053, -0.6070])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "x.requires_grad_(False) # in-place operation\n",
    "print(x) # requires_grad = True가 이제 보이지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1252,  0.7873, -2.5480], requires_grad=True)\n",
      "tensor([-0.1252,  0.7873, -2.5480], requires_grad=True)\n",
      "tensor([-0.1252,  0.7873, -2.5480])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "x.detach() # requires_grad = False인 tensor를 return함.\n",
    "print(x) \n",
    "\n",
    "y  = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. with torch.no_grad() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0767, -0.4190, -0.4595], requires_grad=True)\n",
      "tensor([2.0767, 1.5810, 1.5405], grad_fn=<AddBackward0>)\n",
      "tensor([2.0767, 1.5810, 1.5405])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad() : # with문 안에서는 gradient를 계산하지 않음.\n",
    "    y = x + 2\n",
    "    print(y) # grad_fn=<AddBackward0>이 여기서는 나오지 않음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .backward() 주의사항 : (acuumulated grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# .backward() will accumulate gradients into .grad instead of overwriting\n",
    "# pytorch에서는 gradient를 더해주는 것이 default임.\n",
    "# 그래서 정확한 gradient를 얻지 못하게 된다.\n",
    "for epoch in range(3) :\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# 따라서 정확한 gradient를 얻기 위해서는 .backward()를 호출할 때마다 .grad를 0으로 초기화해줘야 함.\n",
    "for epoch in range(3) :\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad) \n",
    "    \n",
    "    weights.grad.zero_() # gradient를 0으로 초기화해줘야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
