{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8302\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 04:36:56.703207: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-19 04:36:56.755093: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 04:36:57.418871: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-19 04:36:57.449289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 04:36:57.450411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 04:36:57.451123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 04:36:58.172090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 04:36:58.172774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 04:36:58.173403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 04:36:58.174023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 20838 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# tensorflow import and cuda setup\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "device = tf.test.gpu_device_name()\n",
    "if device != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms\n",
    "transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, ), (0.5, ))]\n",
    ")\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('/home/hslee/Desktop/Datasets',\n",
    "                                             download=True,\n",
    "                                             train=True,\n",
    "                                             transform=transforms)\n",
    "testset = torchvision.datasets.FashionMNIST('/home/hslee/Desktop/Datasets',\n",
    "                                             download=True,\n",
    "                                             train=False,\n",
    "                                             transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# image data shape\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functino to show an image\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False) : \n",
    "    if one_channel :\n",
    "        img = img.mean(dim = 0)\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel :\n",
    "        plt.imshow(npimg, cmap='Greys')\n",
    "    else :\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module) :\n",
    "    def __init__(self) : # Net Class에 대한 constructor에는\n",
    "        super(Net, self).__init__()  # nn.Moudle의 constructor를 포함\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # (1, 28, 28) -> (6, 24, 24)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # (6, 24, 24) -> (6, 12, 12)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # (6, 12, 12) -> (16, 8, 8)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120) # (16 * 4 * 4) -> (120)\n",
    "        self.fc2 = nn.Linear(120, 84) # (120) -> (84)\n",
    "        self.fc3 = nn.Linear(84, 10) # (84) -> (10)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" \n",
    "writer = SummaryWriter('./runs/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PIL.Image' has no attribute 'ANTIALIAS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m matplotlib_imshow(img_grid, one_channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# write to tensorboard\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m4samples_fashion_mnist_images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_grid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py:614\u001b[0m, in \u001b[0;36mSummaryWriter.add_image\u001b[0;34m(self, tag, img_tensor, global_step, walltime, dataformats)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m workspace\n\u001b[1;32m    612\u001b[0m     img_tensor \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mFetchBlob(img_tensor)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(\n\u001b[0;32m--> 614\u001b[0m     \u001b[43mimage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataformats\u001b[49m\u001b[43m)\u001b[49m, global_step, walltime\n\u001b[1;32m    615\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:441\u001b[0m, in \u001b[0;36mimage\u001b[0;34m(tag, tensor, rescale, dataformats)\u001b[0m\n\u001b[1;32m    439\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    440\u001b[0m tensor \u001b[38;5;241m=\u001b[39m (tensor \u001b[38;5;241m*\u001b[39m scale_factor)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m--> 441\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mmake_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Summary(value\u001b[38;5;241m=\u001b[39m[Summary\u001b[38;5;241m.\u001b[39mValue(tag\u001b[38;5;241m=\u001b[39mtag, image\u001b[38;5;241m=\u001b[39mimage)])\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:486\u001b[0m, in \u001b[0;36mmake_image\u001b[0;34m(tensor, rescale, rois, labels)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rois \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     image \u001b[38;5;241m=\u001b[39m draw_boxes(image, rois, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m--> 486\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize((scaled_width, scaled_height), \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mANTIALIAS\u001b[49m)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m    489\u001b[0m output \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'ANTIALIAS'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('4samples_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(trainset.data.shape)\n",
    "print(trainset.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_probs(net, images) :\n",
    "    output = net(images)\n",
    "    \n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "def plot_classes_preds(net, images, labels) :\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            # writer.add_figure('predictions vs. actuals',\n",
    "            #                 plot_classes_preds(net, inputs, labels),\n",
    "            #                 global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probs = []\n",
    "class_label = []\n",
    "\n",
    "with torch.no_grad() :\n",
    "    for data in testloader :\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "        \n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "        \n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0) :\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "    \n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "for i in range(len(classes)) :\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
